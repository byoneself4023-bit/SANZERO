#!/usr/bin/env python3
"""
AdvancedCaseSearcher - ê³ ê¸‰ íŒë¡€ ê²€ìƒ‰ ì„œë¹„ìŠ¤
Test_casePedia.ipynbì˜ WorkInjuryCaseSearcherë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë™ì  ì„ê³„ê°’ êµ¬í˜„
"""

import os
import joblib
import pandas as pd
import numpy as np
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity

# í˜•íƒœì†Œ ë¶„ì„ê¸° import (ì¡°ê±´ë¶€)
try:
    from konlpy.tag import Mecab, Okt
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False
    logging.warning("KoNLPy not available, using basic tokenization")

logger = logging.getLogger(__name__)

@dataclass
class AdvancedPrecedentResult:
    """ê³ ê¸‰ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼"""
    case_id: str
    title: str
    content: str
    court: str
    date: str
    similarity: float
    similarity_pct: float
    category: str
    keywords: List[str]
    match_keywords: str
    worker_favorable: str  # ìœ ë¦¬ O / ì• ë§¤ â–³ / ë¶ˆë¦¬ X
    favorability_score: Dict[str, float]  # ì„¸ë¶€ ì ìˆ˜
    accnum: str = ""  # ì‚¬ê±´ë²ˆí˜¸ (ì—°ë„ ì¶”ì¶œìš©)
    kinda: str = ""   # ìŸì  ì •ë³´


@dataclass
class DynamicThresholdResult:
    """ë™ì  ì„ê³„ê°’ ê³„ì‚° ê²°ê³¼"""
    final_threshold: float
    base_threshold: float
    adjustments: Dict[str, float]
    reasoning: Dict[str, str]
    query_analysis: Dict[str, Any]


class AdvancedCaseSearcher:
    """
    ê³ ê¸‰ íŒë¡€ ê²€ìƒ‰ ì„œë¹„ìŠ¤

    íŠ¹ì§•:
    - ë™ì  ì„ê³„ê°’ ê³„ì‚° (ì¼€ì´ìŠ¤ë³„ ìµœì í™”)
    - í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ë° ë„ë©”ì¸ ê°€ì¤‘ì¹˜
    - ê·¼ë¡œì ìœ ë¶ˆë¦¬ ë¶„ì„
    - ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    """

    def __init__(self):
        self.df = None
        self.tokenizer = None
        self.vectorizer = None
        self.tfidf_matrix = None
        self.config = None
        self.is_loaded = False

        # ì„¤ì •ê°’ë“¤
        self.favorable_keywords = []
        self.unfavorable_keywords = []
        self.domain_keywords = []
        self.stopwords = []
        self.boost_multiplier = 2.0

        # ì´ˆê¸°í™”
        self._initialize()

    def _initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self._load_model()
            self._initialize_tokenizer()
            self.is_loaded = True
            logger.info("AdvancedCaseSearcher initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize AdvancedCaseSearcher: {e}")
            self.is_loaded = False

    def _load_model(self):
        """ëª¨ë¸ íŒŒì¼ ë¡œë“œ (joblib ë°©ì‹ í†µì¼)"""
        model_path = Path(__file__).parent.parent / "searcher_model.pkl"

        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found: {model_path}")

        logger.info(f"Loading model from: {model_path}")

        # joblibìœ¼ë¡œ ë¡œë“œ (Basic searcherì™€ ë™ì¼í•œ ë°©ì‹)
        try:
            logger.info("Loading with joblib (unified method)...")
            model_data = joblib.load(model_path)
        except Exception as e:
            logger.error(f"Failed to load with joblib: {e}")
            raise

        # ë°ì´í„° ë¡œë“œ
        self.df = model_data['df']
        self.vectorizer = model_data['vectorizer']
        self.tfidf_matrix = model_data['tfidf_matrix']
        self.config = model_data['config']

        # ì„¤ì • ì ìš©
        self.favorable_keywords = self.config['favorable_keywords']
        self.unfavorable_keywords = self.config['unfavorable_keywords']
        self.domain_keywords = self.config['domain_keywords']
        self.stopwords = self.config['stopwords']
        self.boost_multiplier = self.config['boost_multiplier']

        logger.info(f"Model loaded: {len(self.df):,} precedents, "
                   f"TF-IDF matrix shape: {self.tfidf_matrix.shape}")

    def _initialize_tokenizer(self):
        """í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        if not KONLPY_AVAILABLE:
            logger.warning("Using basic tokenization (KoNLPy not available)")
            self.tokenizer = None
            return

        try:
            self.tokenizer = Mecab()
            logger.info("Mecab tokenizer initialized")
        except:
            try:
                self.tokenizer = Okt()
                logger.info("Okt tokenizer initialized")
            except:
                logger.warning("Failed to initialize KoNLPy tokenizers, using basic tokenization")
                self.tokenizer = None

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text):
            return ""
        return ' '.join(re.sub(r'[^ê°€-í£a-zA-Z0-9\s]+', ' ', str(text)).split())

    def _tokenize(self, text: str) -> List[str]:
        """í† í°í™”"""
        if not text:
            return []

        try:
            if self.tokenizer:
                tokens = self.tokenizer.pos(text)
                words = [w for w, p in tokens if p == 'Noun' and len(w) > 1 and w not in self.stopwords]
                # ë„ë©”ì¸ í‚¤ì›Œë“œ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                domain_words = [w for w in words if w in self.domain_keywords]
                return list(set(words + domain_words))
            else:
                # ê¸°ë³¸ í† í°í™”
                words = [w for w in text.split() if len(w) > 1 and w not in self.stopwords]
                return words
        except Exception as e:
            logger.warning(f"Tokenization failed, using basic split: {e}")
            return [w for w in text.split() if len(w) > 1]

    def calculate_dynamic_threshold(self, query: str, user_preferences: Dict[str, Any] = None) -> DynamicThresholdResult:
        """
        ë™ì  ì„ê³„ê°’ ê³„ì‚°

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            user_preferences: ì‚¬ìš©ì ì„ í˜¸ë„ ì„¤ì •

        Returns:
            DynamicThresholdResult: ê³„ì‚°ëœ ì„ê³„ê°’ ë° ê·¼ê±°
        """
        base_threshold = 0.1  # ê¸°ë³¸ ì„ê³„ê°’
        adjustments = {}
        reasoning = {}

        # 1. ì¿¼ë¦¬ í’ˆì§ˆ ë¶„ì„
        query_analysis = self._analyze_query_quality(query)

        # 2. ì¿¼ë¦¬ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        query_length = len(query.split())
        if query_length <= 3:
            length_adjustment = -0.05  # ì§§ì€ ì¿¼ë¦¬ëŠ” ì„ê³„ê°’ ë‚®ì¶¤
            reasoning["length"] = f"ì§§ì€ ì¿¼ë¦¬({query_length}ë‹¨ì–´)ë¡œ ì„ê³„ê°’ ì™„í™”"
        elif query_length >= 10:
            length_adjustment = 0.15  # ê¸´ ì¿¼ë¦¬ëŠ” ì„ê³„ê°’ ë†’ì„
            reasoning["length"] = f"ìƒì„¸í•œ ì¿¼ë¦¬({query_length}ë‹¨ì–´)ë¡œ ì„ê³„ê°’ ìƒí–¥"
        else:
            length_adjustment = 0.0
            reasoning["length"] = f"ì ì • ê¸¸ì´ ì¿¼ë¦¬({query_length}ë‹¨ì–´)"

        adjustments["length"] = length_adjustment

        # 3. ë„ë©”ì¸ í‚¤ì›Œë“œ ë°€ë„ ê¸°ë°˜ ì¡°ì •
        query_tokens = set(self._tokenize(query))
        domain_ratio = len(query_tokens & set(self.domain_keywords)) / max(len(query_tokens), 1)

        if domain_ratio > 0.3:
            domain_adjustment = 0.1
            reasoning["domain"] = f"ë„ë©”ì¸ í‚¤ì›Œë“œ ë¹„ìœ¨ ë†’ìŒ({domain_ratio:.1%})"
        elif domain_ratio > 0.1:
            domain_adjustment = 0.05
            reasoning["domain"] = f"ë„ë©”ì¸ í‚¤ì›Œë“œ ì ì •({domain_ratio:.1%})"
        else:
            domain_adjustment = -0.05
            reasoning["domain"] = f"ë„ë©”ì¸ í‚¤ì›Œë“œ ë¶€ì¡±({domain_ratio:.1%})"

        adjustments["domain"] = domain_adjustment

        # 4. ì‚¬ìš©ì ì„ í˜¸ë„ ë°˜ì˜
        if user_preferences:
            accuracy_level = user_preferences.get("accuracy_level", "medium")
            if accuracy_level == "high":
                preference_adjustment = 0.2
                reasoning["preference"] = "ë†’ì€ ì •í™•ë„ ì„ í˜¸"
            elif accuracy_level == "low":
                preference_adjustment = -0.15
                reasoning["preference"] = "ê´€ë ¨ì„± ìš°ì„  ì„ í˜¸"
            else:
                preference_adjustment = 0.0
                reasoning["preference"] = "í‘œì¤€ ì •í™•ë„"
        else:
            preference_adjustment = 0.0
            reasoning["preference"] = "ê¸°ë³¸ ì„¤ì •"

        adjustments["preference"] = preference_adjustment

        # 5. ì˜ˆìƒ ê²°ê³¼ ë¶„í¬ ê¸°ë°˜ ì¡°ì • (ê°„ë‹¨í•œ ì¶”ì •)
        if query_analysis["has_specific_terms"]:
            distribution_adjustment = 0.05
            reasoning["distribution"] = "êµ¬ì²´ì  ìš©ì–´ë¡œ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ ì˜ˆìƒ"
        else:
            distribution_adjustment = -0.05
            reasoning["distribution"] = "ì¼ë°˜ì  ìš©ì–´ë¡œ ê²°ê³¼ í™•ì‚° ì˜ˆìƒ"

        adjustments["distribution"] = distribution_adjustment

        # 6. ìµœì¢… ì„ê³„ê°’ ê³„ì‚°
        total_adjustment = sum(adjustments.values())
        final_threshold = max(0.05, min(0.8, base_threshold + total_adjustment))

        # ì„¤ëª… ìƒì„±
        main_reasoning = []
        for key, value in adjustments.items():
            if abs(value) > 0.01:  # ìœ ì˜ë¯¸í•œ ì¡°ì •ë§Œ í‘œì‹œ
                direction = "ìƒí–¥" if value > 0 else "í•˜í–¥"
                main_reasoning.append(f"{reasoning[key]} ({direction} {abs(value):.2f})")

        threshold_explanation = f"ê¸°ë³¸ ì„ê³„ê°’ {base_threshold} â†’ ìµœì¢… {final_threshold:.3f}"
        if main_reasoning:
            threshold_explanation += f" ({', '.join(main_reasoning)})"

        return DynamicThresholdResult(
            final_threshold=final_threshold,
            base_threshold=base_threshold,
            adjustments=adjustments,
            reasoning={
                "threshold_explanation": threshold_explanation,
                **reasoning
            },
            query_analysis=query_analysis
        )

    def _analyze_query_quality(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ í’ˆì§ˆ ë¶„ì„"""
        tokens = self._tokenize(query)

        # í‚¤ì›Œë“œ ìœ í˜• ë¶„ì„
        legal_terms = ["ì‚¬ê³ ", "ì¬í•´", "ë¶€ìƒ", "ì†í•´", "ë°°ìƒ", "ì±…ì„", "ê³¼ì‹¤", "ì•ˆì „", "ì‘ì—…", "ê·¼ë¬´"]
        specific_terms = ["í”„ë ˆìŠ¤", "ì¶”ë½", "ì ˆë‹¨", "í™”ìƒ", "ë¼ì„", "ì¶©ëŒ", "ê°ì „", "ì¤‘ë…"]

        has_legal_terms = any(term in query for term in legal_terms)
        has_specific_terms = any(term in query for term in specific_terms)

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        quality_score = 0.5  # ê¸°ë³¸ê°’

        if has_legal_terms:
            quality_score += 0.2
        if has_specific_terms:
            quality_score += 0.2
        if len(tokens) >= 5:
            quality_score += 0.1

        return {
            "token_count": len(tokens),
            "has_legal_terms": has_legal_terms,
            "has_specific_terms": has_specific_terms,
            "quality_score": min(quality_score, 1.0),
            "complexity": "high" if len(tokens) > 8 else "medium" if len(tokens) > 4 else "low"
        }

    def search(self, query: str, top_k: int = 10,
               user_preferences: Dict[str, Any] = None) -> List[AdvancedPrecedentResult]:
        """
        ê³ ê¸‰ íŒë¡€ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            user_preferences: ì‚¬ìš©ì ì„ í˜¸ë„ ì„¤ì •

        Returns:
            List[AdvancedPrecedentResult]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_loaded:
            logger.error("Searcher not loaded properly")
            return []

        # 1. ë™ì  ì„ê³„ê°’ ê³„ì‚°
        threshold_result = self.calculate_dynamic_threshold(query, user_preferences)
        dynamic_threshold = threshold_result.final_threshold

        logger.info(f"Dynamic threshold: {dynamic_threshold:.3f} for query: '{query[:50]}...'")

        # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”
        query_clean = self._clean_text(query)
        query_tokens = self._tokenize(query_clean)
        query_str = ' '.join(query_tokens)
        query_set = set(query_tokens)

        # 3. TF-IDF ìœ ì‚¬ë„ ê³„ì‚°
        query_vec = self.vectorizer.transform([query_str])
        base_similarities = cosine_similarity(query_vec, self.tfidf_matrix)[0]

        # 4. í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš©
        boosted_similarities = self._apply_keyword_boosting(
            base_similarities, query_set, query_tokens
        )

        # 5. ë™ì  ì„ê³„ê°’ í•„í„°ë§
        valid_indices = np.where(boosted_similarities >= dynamic_threshold)[0]
        if len(valid_indices) == 0:
            # ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„ê³„ê°’ìœ¼ë¡œ ì¬ì‹œë„
            logger.warning(f"No results above dynamic threshold {dynamic_threshold:.3f}, using base threshold")
            valid_indices = np.where(boosted_similarities >= 0.05)[0]

        # 6. ìƒìœ„ ê²°ê³¼ ì„ íƒ
        if len(valid_indices) > top_k:
            top_indices = valid_indices[np.argsort(boosted_similarities[valid_indices])[-top_k:]][::-1]
        else:
            top_indices = valid_indices[np.argsort(boosted_similarities[valid_indices])[::-1]]

        # 7. ê²°ê³¼ ìƒì„±
        results = []
        for i, idx in enumerate(top_indices):
            row = self.df.iloc[idx]
            similarity_score = boosted_similarities[idx]

            # ë§¤ì¹­ í‚¤ì›Œë“œ ì¶”ì¶œ
            match_keywords = self._extract_matching_keywords(query_set, row)

            # ê·¼ë¡œì ìœ ë¶ˆë¦¬ ë¶„ì„
            favorability_analysis = self._analyze_worker_favorability(row['noncontent'])

            result = AdvancedPrecedentResult(
                case_id=str(row.get('accnum', f'CASE_{idx}')),  # accnumì„ case_idë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ CASE_{idx}
                title=row['title'],
                content=row['noncontent'][:1000],  # ì²˜ìŒ 1000ìë§Œ
                court=row['courtname'],
                date=str(row.get('date', 'Unknown')),
                similarity=round(similarity_score, 4),
                similarity_pct=round(similarity_score * 100, 2),
                category=row.get('category', 'ê¸°íƒ€'),  # ì‚¬ê³  ìœ í˜•
                keywords=query_tokens[:5],  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
                match_keywords=match_keywords,
                worker_favorable=favorability_analysis["assessment"],
                favorability_score=favorability_analysis["scores"],
                accnum=str(row.get('accnum', '')),  # ì‚¬ê±´ë²ˆí˜¸ (ì—°ë„ ì¶”ì¶œìš©)
                kinda=str(row.get('kinda', ''))     # ìŸì  ì •ë³´
            )
            results.append(result)

        logger.info(f"Found {len(results)} results above threshold {dynamic_threshold:.3f}")
        return results

    def _apply_keyword_boosting(self, base_similarities: np.ndarray,
                              query_set: set, query_tokens: List[str]) -> np.ndarray:
        """í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš©"""
        boosted = base_similarities.copy()

        for i, similarity in enumerate(base_similarities):
            try:
                doc_tokens = set(self.df.iloc[i]['tokens'])
                common_keywords = query_set & doc_tokens

                if common_keywords:
                    # ë„ë©”ì¸ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
                    domain_count = sum(1 for kw in common_keywords if kw in self.domain_keywords)

                    # ë§¤ì¹­ ë¹„ìœ¨
                    match_ratio = len(common_keywords) / max(len(query_tokens), 1)

                    # ë¶€ìŠ¤íŒ… ê³„ì‚°
                    boost_factor = match_ratio * (1 + domain_count * 0.2) * (self.boost_multiplier - 1)
                    boosted[i] = min(similarity * (1 + boost_factor), 1.0)
            except Exception as e:
                # ì—ëŸ¬ ë°œìƒ ì‹œ ì›ë³¸ ìœ ì‚¬ë„ ìœ ì§€
                continue

        return boosted

    def _extract_matching_keywords(self, query_set: set, row: pd.Series) -> str:
        """ë§¤ì¹­ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            doc_tokens = set(row['tokens'])
            common = (query_set & doc_tokens) - set(self.stopwords)

            # ë„ë©”ì¸ í‚¤ì›Œë“œ ìš°ì„ 
            domain_keywords = [kw for kw in common if kw in self.domain_keywords]
            other_keywords = [kw for kw in common if kw not in self.domain_keywords]

            # ìƒìœ„ 5ê°œ ì„ íƒ
            top_keywords = (domain_keywords + other_keywords)[:5]
            return ', '.join(top_keywords) if top_keywords else '-'
        except:
            return '-'

    def _analyze_worker_favorability(self, content: str) -> Dict[str, Any]:
        """ê·¼ë¡œì ìœ ë¶ˆë¦¬ ë¶„ì„"""
        if pd.isna(content):
            return {"assessment": "ì• ë§¤ â–³", "scores": {"favorable": 0, "unfavorable": 0}}

        text = str(content).lower()

        # ê²°ë¡  ë¶€ë¶„ ê°€ì¤‘ì¹˜ (ë§ˆì§€ë§‰ 30%)
        text_length = len(text)
        conclusion = text[int(text_length * 0.7):]

        # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        favorable_score = 0
        unfavorable_score = 0

        # ê²°ë¡  ë¶€ë¶„ (ê°€ì¤‘ì¹˜ 2ë°°)
        for keyword in self.favorable_keywords:
            favorable_score += conclusion.lower().count(keyword.lower()) * 2

        for keyword in self.unfavorable_keywords:
            unfavorable_score += conclusion.lower().count(keyword.lower()) * 2

        # ì „ì²´ ë¬¸ì„œ
        for keyword in self.favorable_keywords:
            favorable_score += text.count(keyword.lower())

        for keyword in self.unfavorable_keywords:
            unfavorable_score += text.count(keyword.lower())

        # íŒë‹¨
        if favorable_score > unfavorable_score + 2:
            assessment = "ìœ ë¦¬ O"
        elif unfavorable_score > favorable_score + 2:
            assessment = "ë¶ˆë¦¬ X"
        else:
            assessment = "ì• ë§¤ â–³"

        return {
            "assessment": assessment,
            "scores": {
                "favorable": favorable_score,
                "unfavorable": unfavorable_score
            }
        }

    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.is_loaded:
            return {"status": "not_loaded"}

        return {
            "status": "loaded",
            "total_precedents": len(self.df),
            "tfidf_matrix_shape": self.tfidf_matrix.shape,
            "vocabulary_size": len(self.vectorizer.vocabulary_),
            "favorable_keywords": len(self.favorable_keywords),
            "unfavorable_keywords": len(self.unfavorable_keywords),
            "domain_keywords": len(self.domain_keywords),
            "boost_multiplier": self.boost_multiplier,
            "tokenizer": "Mecab" if self.tokenizer.__class__.__name__ == "Mecab" else "Okt" if self.tokenizer else "Basic"
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_advanced_searcher_instance: Optional[AdvancedCaseSearcher] = None

def get_advanced_searcher() -> AdvancedCaseSearcher:
    """AdvancedCaseSearcher ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _advanced_searcher_instance

    if _advanced_searcher_instance is None:
        _advanced_searcher_instance = AdvancedCaseSearcher()

    return _advanced_searcher_instance


# í¸ì˜ í•¨ìˆ˜ë“¤
def search_with_dynamic_threshold(
    query: str,
    top_k: int = 10,
    accuracy_level: str = "medium"
) -> Tuple[List[AdvancedPrecedentResult], DynamicThresholdResult]:
    """ë™ì  ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ê²€ìƒ‰"""
    searcher = get_advanced_searcher()

    if not searcher.is_loaded:
        return [], None

    user_preferences = {"accuracy_level": accuracy_level}
    threshold_result = searcher.calculate_dynamic_threshold(query, user_preferences)
    results = searcher.search(query, top_k, user_preferences)

    return results, threshold_result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    def test_advanced_searcher():
        searcher = AdvancedCaseSearcher()

        if not searcher.is_loaded:
            print("âŒ ê²€ìƒ‰ê¸° ë¡œë“œ ì‹¤íŒ¨")
            return

        print("ğŸ“Š ê²€ìƒ‰ê¸° í†µê³„:")
        stats = searcher.get_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")

        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        query = "ì‘ì—… ì¤‘ í”„ë ˆìŠ¤ ê¸°ê³„ì— ì†ê°€ë½ ë¼ì„"
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{query}'")

        # ë™ì  ì„ê³„ê°’ ê³„ì‚°
        threshold_result = searcher.calculate_dynamic_threshold(
            query,
            {"accuracy_level": "medium"}
        )
        print(f"\nğŸ“Š ë™ì  ì„ê³„ê°’: {threshold_result.final_threshold:.3f}")
        print(f"   ì„¤ëª…: {threshold_result.reasoning['threshold_explanation']}")

        # ê²€ìƒ‰ ì‹¤í–‰
        results = searcher.search(query, top_k=5, user_preferences={"accuracy_level": "medium"})

        print(f"\nğŸ“‹ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´")
        for i, result in enumerate(results, 1):
            print(f"{i}. [{result.similarity_pct:5.1f}%] {result.worker_favorable} | {result.title[:50]}...")

    test_advanced_searcher()