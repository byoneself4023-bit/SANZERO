#!/usr/bin/env python3
"""
Simple Search Service - Test_casePedia.ipynb ì •í™•í•œ ë°©ì‹ êµ¬í˜„
ê¸°ì¡´ ë³µì¡í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ëŒ€ì²´í•˜ëŠ” ë‹¨ìˆœí•˜ê³  í™•ì‹¤í•œ ê²€ìƒ‰ ì„œë¹„ìŠ¤
"""

import joblib
import pandas as pd
import numpy as np
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ (ëª¨ë¸ ìºì‹±ìš©)
_model_loaded = False
_df = None
_vectorizer = None
_tfidf_matrix = None
_config = None

def load_searcher_model_direct() -> bool:
    """
    Test_casePedia ë°©ì‹ìœ¼ë¡œ searcher_model.pkl ì§ì ‘ ë¡œë“œ
    ë³µì¡í•œ í´ë˜ìŠ¤ êµ¬ì¡° ì—†ì´ ë‹¨ìˆœí•˜ê²Œ ëª¨ë¸ë§Œ ë¡œë“œ
    """
    global _model_loaded, _df, _vectorizer, _tfidf_matrix, _config

    if _model_loaded:
        return True

    try:
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        model_path = Path(__file__).parent.parent / "searcher_model.pkl"

        if not model_path.exists():
            logger.error(f"Model file not found: {model_path}")
            return False

        logger.info(f"Loading model directly from: {model_path}")

        # joblibìœ¼ë¡œ ì§ì ‘ ë¡œë“œ (Test_casePedia ë°©ì‹)
        model_data = joblib.load(model_path)

        # ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹
        _df = model_data['df']
        _vectorizer = model_data['vectorizer']
        _tfidf_matrix = model_data['tfidf_matrix']
        _config = model_data.get('config', {})

        # ë¡œë“œ ì„±ê³µ í”Œë˜ê·¸
        _model_loaded = True

        logger.info(f"âœ… Simple model loaded successfully: {len(_df):,} cases, "
                   f"vocabulary: {len(_vectorizer.vocabulary_):,}")

        return True

    except Exception as e:
        logger.error(f"âŒ Failed to load simple model: {e}")
        return False


def search_precedents_simple(query: str, top_k: int = 10) -> List[Dict[str, Any]]:
    """
    Test_casePedia.ipynbì˜ ì •í™•í•œ ê²€ìƒ‰ ë°©ì‹
    ìµœì†Œí•œì˜ ì „ì²˜ë¦¬, ì§ì ‘ì ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    """
    # ëª¨ë¸ ë¡œë“œ í™•ì¸
    if not _model_loaded and not load_searcher_model_direct():
        logger.error("âŒ Model could not be loaded")
        return []

    try:
        logger.info(f"Simple search started: query='{query}', top_k={top_k}")

        # 1. ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ (Test_casePedia ë°©ì‹)
        query_clean = re.sub(r'[^\w\s]', ' ', query)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        query_clean = ' '.join(query_clean.split())    # ê³µë°± ì •ê·œí™”

        logger.info(f"Query preprocessed: '{query}' â†’ '{query_clean}'")

        # 2. ë²¡í„°í™” (ì§ì ‘ì ìœ¼ë¡œ)
        query_vector = _vectorizer.transform([query_clean])

        logger.info(f"Query vectorized: shape={query_vector.shape}")

        # 3. ìœ ì‚¬ë„ ê³„ì‚° (Test_casePedia ì •í™•í•œ ë°©ì‹)
        similarities = cosine_similarity(query_vector, _tfidf_matrix)[0]

        # ìœ ì‚¬ë„ í†µê³„ ë¡œê¹…
        max_sim = np.max(similarities)
        mean_sim = np.mean(similarities)
        nonzero_count = np.count_nonzero(similarities > 0.001)

        logger.info(f"Similarities calculated: max={max_sim:.4f}, mean={mean_sim:.4f}, "
                   f"nonzero_count={nonzero_count}")

        # 4. ìƒìœ„ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]

        # 5. ê²°ê³¼ êµ¬ì„± (ë‹¨ê³„ì  í•„í„°ë§ ì ìš©)
        results = []
        raw_results = []

        # ë¨¼ì € ëª¨ë“  ì˜ë¯¸ìˆëŠ” ê²°ê³¼ ìˆ˜ì§‘
        for i, idx in enumerate(top_indices):
            row = _df.iloc[idx]
            similarity = similarities[idx]

            # Test_casePedia ë°©ì‹: ë‚®ì€ ì„ê³„ê°’ (0.1%)
            if similarity >= 0.001:
                title = str(row.get('title', 'Unknown Title'))
                court = str(row.get('courtname', 'Unknown Court'))

                # ë‚´ìš© ìš”ì•½ (ê°€ë…ì„± ê°œì„ )
                content = str(row.get('noncontent', ''))
                content_summary = _create_readable_summary(content)

                # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¹´í…Œê³ ë¦¬ ì ìš©
                friendly_category = get_friendly_category(title, content)

                # ì‹¤ì œ ì»¬ëŸ¼ êµ¬ì¡° ê¸°ë°˜ ìˆ˜ì •
                # kindaë¥¼ ë‚ ì§œë¡œ ì‚¬ìš© (ì‹¤ì œ ë°ì´í„°ì—ì„œ kindaê°€ ë‚ ì§œ ì •ë³´)
                raw_date = str(row.get('kinda', 'Unknown Date'))
                formatted_date = format_court_date(raw_date)

                # ì œëª© ìš”ì•½
                summarized_title = summarize_case_title(title)

                # ì¶”ê°€ í•„ë“œë“¤ (ì‹¤ì œ ì»¬ëŸ¼ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                kinda = str(row.get('kinda', ''))    # ì‹¤ì œ íŒê²°ê²°ê³¼ (ê¸°ê°, ì¸ìš©, ì·¨í•˜ ë“±)
                kindb = str(row.get('kindb', ''))    # ì‚¬ê±´ ë¶„ì•¼ 1 (ìš”ì–‘, ì¥í•´ ë“±)
                kindc = str(row.get('kindc', ''))    # ì‚¬ê±´ ë¶„ì•¼ 2 (ì—…ë¬´ìƒì‚¬ê³ , ì—…ë¬´ìƒì§ˆë³‘ ë“±)

                # ì—°ë„ ì¶”ì¶œì„ content(noncontent), titleì—ì„œ ì‹œë„ (kindaëŠ” íŒê²°ê²°ê³¼ì´ë¯€ë¡œ ì œì™¸)
                year_info = extract_year_from_text(content, title)

                # ë””ë²„ê¹…ìš© ë¡œê·¸
                logger.info(f"ë°ì´í„° í™•ì¸ - kinda(íŒê²°ê²°ê³¼): {kinda}, kindb: {kindb}, kindc: {kindc}, ì—°ë„: {year_info}")

                result = {
                    'rank': i + 1,
                    'case_id': f"CASE_{idx}",
                    'title': summarized_title,
                    'court': court,
                    'date': formatted_date,
                    'similarity': round(float(similarity), 4),
                    'similarity_pct': round(float(similarity) * 100, 2),
                    'content': content_summary,
                    'category': friendly_category,
                    'accnum': year_info,  # ì—°ë„ ì •ë³´
                    'kinda': kinda,  # ì‹¤ì œ íŒê²°ê²°ê³¼ (ê¸°ê°, ì¸ìš©, ì·¨í•˜ ë“±)
                    'case_type': f"{kindb} {kindc}".strip() if kindb and kindc else kindb or kindc or ''  # ì‚¬ê±´ë¶„ì•¼
                }
                raw_results.append(result)

        logger.info(f"Raw results found: {len(raw_results)}")

        # ë‹¨ê³„ì  í’ˆì§ˆ í•„í„°ë§
        for result in raw_results:
            # 1ì°¨ í•„í„°ë§: Unknown ë°ì´í„° ì œì™¸ (ì„ íƒì )
            if result['court'] == 'Unknown Court':
                continue

            # 2ì°¨ í•„í„°ë§: ê¸°ê° íŒë¡€ ì œì™¸ (ì„ íƒì )
            if "ê¸°ê°" in result['title'].lower():
                continue

            # í•„í„°ë§ í†µê³¼í•œ ê²°ê³¼ ì¶”ê°€
            result['rank'] = len(results) + 1  # í•„í„°ë§ í›„ ìˆœìœ„ ì¬ì¡°ì •
            results.append(result)

        logger.info(f"Filtered results: {len(results)}")

        # í´ë°± ë©”ì»¤ë‹ˆì¦˜: í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì™„í™”
        if len(results) < 3 and len(raw_results) >= 3:
            logger.info("Applying fallback mechanism due to insufficient filtered results")
            results = raw_results[:top_k]  # ìƒìœ„ ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©

        # ì²« ë²ˆì§¸ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        if results:
            top_result = results[0]
            logger.info(f"Top result: similarity={top_result['similarity']:.4f} "
                       f"({top_result['similarity_pct']:.2f}%), "
                       f"title='{top_result['title'][:50]}...', court='{top_result['court']}'")

        logger.info(f"âœ… Simple search completed: found {len(results)} relevant results")
        return results

    except Exception as e:
        logger.error(f"âŒ Simple search failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []

def get_simple_search_stats() -> Dict[str, Any]:
    """ëª¨ë¸ ìƒíƒœ ë° í†µê³„ ì •ë³´ ë°˜í™˜"""
    if not _model_loaded:
        return {
            "status": "not_loaded",
            "message": "Model not loaded"
        }

    try:
        return {
            "status": "loaded",
            "total_cases": len(_df),
            "vocabulary_size": len(_vectorizer.vocabulary_),
            "tfidf_matrix_shape": _tfidf_matrix.shape,
            "available_columns": list(_df.columns),
            "sample_titles": _df['title'].head(3).tolist() if 'title' in _df.columns else [],
            "model_type": "simple_direct"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

def extract_year_from_text(*texts: str) -> str:
    """í…ìŠ¤íŠ¸ë“¤ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
    import re

    logger.debug(f"ì—°ë„ ì¶”ì¶œ ì‹œë„ - ì…ë ¥ í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(texts)}")

    for i, text in enumerate(texts):
        if not text or str(text).strip() == '' or str(text) == 'nan':
            logger.debug(f"í…ìŠ¤íŠ¸ {i+1}: ë¹ˆ ë¬¸ìì—´ ìŠ¤í‚µ")
            continue

        text = str(text).strip()
        logger.debug(f"í…ìŠ¤íŠ¸ {i+1} ë¶„ì„ ì¤‘: '{text[:50]}...'")

        # íŒë¡€ ë¬¸ì„œì— íŠ¹í™”ëœ í™•ì¥ëœ ì—°ë„ íŒ¨í„´
        year_patterns = [
            # 1ìˆœìœ„: í•œêµ­ ë²•ì› ì‚¬ê±´ë²ˆí˜¸ íŒ¨í„´ (ê°€ì¥ í™•ì‹¤í•œ ì—°ë„ ì •ë³´)
            r'(\d{4})ê³ í•©\d*',     # 2018ê³ í•©123
            r'(\d{4})ê³ ë‹¨\d*',     # 2019ê³ ë‹¨456
            r'(\d{4})ë‚˜\d*',       # 2020ë‚˜789
            r'(\d{4})ë‹¤\d*',       # 2021ë‹¤123
            r'(\d{4})ëˆ„\d*',       # í–‰ì •ì†Œì†¡
            r'(\d{4})ì•„\d*',       # íŠ¹ë³„ë²•
            r'(\d{4})ë…¸\d*',       # í˜•ì‚¬ì†Œì†¡
            r'(\d{4})êµ¬\d*',       # êµ¬ë‹¨
            r'(\d{4})ì´ˆ\d*',       # ì´ˆê¸°ê²°ì •
            r'(\d{4})ì¬\d*',       # ì¬ì‹¬

            # 2ìˆœìœ„: íŒê²°/ì„ ê³  ê´€ë ¨ íŒ¨í„´
            r'(\d{4})ë…„.*?ì„ ê³ ',     # 2022ë…„ 3ì›” 15ì¼ ì„ ê³ 
            r'(\d{4})ë…„.*?íŒê²°',     # 2021ë…„ íŒê²°
            r'ì„ ê³ .*?(\d{4})ë…„',     # ì„ ê³  2020ë…„
            r'íŒê²°.*?(\d{4})ë…„',     # íŒê²° 2019ë…„

            # 3ìˆœìœ„: ì¼ë°˜ì ì¸ ë‚ ì§œ íŒ¨í„´
            r'(\d{4})[ë…„]',         # 2022ë…„
            r'(\d{4})\.\s*\d',      # 2023. 1
            r'(\d{4})-\d',          # 2024-01
            r'(\d{4})/\d',          # 2024/01

            # 4ìˆœìœ„: ì‚¬ê±´ ê´€ë ¨ íŒ¨í„´
            r'ì‚¬ê±´.*?(\d{4})',      # ì‚¬ê±´ 2020
            r'ì‹ ì²­.*?(\d{4})',      # ì‹ ì²­ 2021
            r'ì²˜ë¶„.*?(\d{4})',      # ì²˜ë¶„ 2019

            # 5ìˆœìœ„: ê¸°íƒ€ íŒ¨í„´ (ëœ í™•ì‹¤í•¨)
            r'(\d{4})\s+\d',        # 2025 01
            r'[^\d](\d{4})[^\d]',   # ì–‘ìª½ì´ ìˆ«ìê°€ ì•„ë‹Œ 4ìë¦¬
            r'^(\d{4})',            # ë¬¸ìì—´ ì‹œì‘ì˜ 4ìë¦¬
            r'(\d{4})'              # ë§ˆì§€ë§‰ í›„ë³´: ì¼ë°˜ì ì¸ 4ìë¦¬ ìˆ«ì
        ]

        for pattern in year_patterns:
            try:
                matches = re.findall(pattern, text)
                logger.debug(f"íŒ¨í„´ '{pattern}' ë§¤ì¹˜ ê²°ê³¼: {matches}")

                for match in matches:
                    try:
                        year = int(match)
                        if 1990 <= year <= 2030:  # ë²”ìœ„ë¥¼ 1990-2030ìœ¼ë¡œ ì¡°ì •
                            logger.debug(f"âœ… ì—°ë„ ì¶”ì¶œ ì„±ê³µ: {year} (íŒ¨í„´: '{pattern}', í…ìŠ¤íŠ¸: '{text[:30]}...')")
                            return str(year)
                    except (ValueError, TypeError):
                        continue
            except Exception as e:
                logger.debug(f"íŒ¨í„´ '{pattern}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

    logger.debug("âŒ ì—°ë„ ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  íŒ¨í„´ì—ì„œ ë§¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    return 'ë¯¸ìƒ'

def get_friendly_category(title: str, content: str) -> str:
    """ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
    title_lower = title.lower()
    content_lower = content.lower()

    # ì œëª©ê³¼ ë‚´ìš©ì„ ëª¨ë‘ ê²€ìƒ‰
    combined_text = f"{title_lower} {content_lower}"

    # ì¶”ë½ì‚¬ê³ 
    if any(word in combined_text for word in ["ì¶”ë½", "ë‚™ìƒ", "ë–¨ì–´", "ë¹„ê³„", "ì‚¬ë‹¤ë¦¬", "ê³ ì†Œ", "ë†’ì´", "ì˜¥ìƒ", "ì•„ë˜ë¡œ"]):
        return "ì¶”ë½ì‚¬ê³ "

    # ê¸°ê³„ì‚¬ê³ 
    elif any(word in combined_text for word in ["ë¼ì„", "ì ˆë‹¨", "í”„ë ˆìŠ¤", "ê¸°ê³„", "í¬ë ˆì¸", "ì»¨ë² ì´ì–´", "ë¼ì—¬", "ì ˆë‹¨ê¸°", "ì••ì°©"]):
        return "ê¸°ê³„ì‚¬ê³ "

    # í™”ì¬ì‚¬ê³ 
    elif any(word in combined_text for word in ["í™”ìƒ", "í™”ì¬", "í­ë°œ", "ìš©ì ‘", "ê³ ì˜¨", "ì¦ê¸°", "í™”ì—¼", "í­ë°œ", "ì—´ìƒ"]):
        return "í™”ì¬ì‚¬ê³ "

    # êµí†µì‚¬ê³ 
    elif any(word in combined_text for word in ["êµí†µ", "ì°¨ëŸ‰", "ì¶©ëŒ", "ì§€ê²Œì°¨", "í™”ë¬¼ì°¨", "íŠ¸ëŸ­", "ë²„ìŠ¤", "ìŠ¹ìš©ì°¨"]):
        return "êµí†µì‚¬ê³ "

    # ì¤‘ë…ì‚¬ê³ 
    elif any(word in combined_text for word in ["ì¤‘ë…", "ì§ˆì‹", "í™”í•™", "ê°€ìŠ¤", "ì•½í’ˆ", "ë…ì„±", "ì¤‘ë…ì„±", "í¡ì…"]):
        return "ì¤‘ë…ì‚¬ê³ "

    # ê°ì „ì‚¬ê³ 
    elif any(word in combined_text for word in ["ê°ì „", "ì „ê¸°", "ì „ì„ ", "ëˆ„ì „", "ì „ë¥˜", "ì „ì••", "ì „ê¸°ì "]):
        return "ê°ì „ì‚¬ê³ "

    # ê¸°íƒ€ ì‚°ì—…ì¬í•´
    else:
        return "ì‚°ì—…ì¬í•´"

def format_court_date(date_str: str) -> str:
    """ë²•ì› ë‚ ì§œë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í¬ë§·"""
    if not date_str or date_str == "Unknown Date":
        return "ë‚ ì§œë¯¸ìƒ"

    # "ì·¨ì†Œ" ë“± ì˜ëª»ëœ ë°ì´í„° ì²˜ë¦¬
    if "ì·¨ì†Œ" in date_str or "ê¸°ê°" in date_str:
        return "ë‚ ì§œë¯¸ìƒ"

    # ê·¸ëŒ€ë¡œ ë°˜í™˜ (pkl íŒŒì¼ì˜ ì›ë³¸ ë‚ ì§œ ë°ì´í„° ì‚¬ìš©)
    return date_str

def summarize_case_title(title: str) -> str:
    """ë³µì¡í•œ íŒë¡€ ì œëª©ì„ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ìš”ì•½"""
    if len(title) > 50:
        return title[:47] + "..."
    return title

def _create_readable_summary(content: str, max_length: int = 500) -> str:
    """ê°€ë…ì„± ì¢‹ì€ ì½˜í…ì¸  ìš”ì•½ ìƒì„±"""
    if not content or content.strip() == "":
        return "ë‚´ìš© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

    # ê¸°ë³¸ ì •ë¦¬
    content = content.strip()

    # ê¸¸ì´ê°€ ì œí•œë³´ë‹¤ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if len(content) <= max_length:
        return content

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
    sentences = content.split('. ')
    result = ""

    for sentence in sentences:
        # ë‹¤ìŒ ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ê°€ ì´ˆê³¼ë˜ëŠ”ì§€ í™•ì¸
        test_result = result + sentence + ". " if result else sentence + ". "

        if len(test_result) <= max_length - 3:  # "..." ê³µê°„ í™•ë³´
            result = test_result
        else:
            break

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ë ¸ìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸€ì ë‹¨ìœ„ë¡œ ìë¥´ê¸°
    if result and len(result) > 100:  # ìµœì†Œí•œì˜ ì˜ë¯¸ìˆëŠ” ê¸¸ì´
        return result.rstrip() + "..."
    else:
        # ê¸€ì ë‹¨ìœ„ë¡œ ìë¥´ë˜, ìì—°ìŠ¤ëŸ¬ìš´ ëë§ºìŒ ì°¾ê¸°
        truncated = content[:max_length]

        # ë§ˆì§€ë§‰ ê³µë°±ì´ë‚˜ êµ¬ë‘ì ì—ì„œ ìë¥´ê¸°
        for i in range(len(truncated) - 1, max(0, len(truncated) - 50), -1):
            if truncated[i] in [' ', '.', ',', 'ë‹¤', 'ê³ ', 'ë©°', 'ìŒ', 'ì„']:
                truncated = truncated[:i + 1]
                break

        return truncated + "..."

def debug_model_structure():
    """pkl íŒŒì¼ì˜ ì •í™•í•œ êµ¬ì¡° ë””ë²„ê·¸ (ê°œë°œìš©)"""
    if not _model_loaded and not load_searcher_model_direct():
        return {"error": "Model not loaded"}

    try:
        debug_info = {
            "dataframe_shape": _df.shape,
            "columns": list(_df.columns),
            "tfidf_matrix_shape": _tfidf_matrix.shape,
            "vocabulary_sample": list(_vectorizer.vocabulary_.keys())[:10],
        }

        # ì²« 3ê°œ í–‰ì˜ ì‹¤ì œ ë°ì´í„° í™•ì¸ (ì—°ë„ ì¶”ì¶œ ë””ë²„ê¹…ìš©) - ëª¨ë“  í•„ë“œ í™•ì¸
        sample_data = []
        for i in range(min(3, len(_df))):
            row = _df.iloc[i]

            # ëª¨ë“  í•„ë“œì˜ ì‹¤ì œ ë‚´ìš© í™•ì¸
            row_data = {"index": i}
            for col in _df.columns:
                value = str(row.get(col, ''))
                # noncontentëŠ” ê¸¸ì´ê°€ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 200ìë¡œ ì œí•œ
                if col == 'noncontent':
                    row_data[col] = value[:200] + ("..." if len(value) > 200 else "")
                else:
                    row_data[col] = value[:150] + ("..." if len(value) > 150 else "")

            sample_data.append(row_data)

        debug_info["sample_data"] = sample_data

        return debug_info

    except Exception as e:
        return {"debug_error": str(e)}

def generate_simple_report(query: str, top_n: int = 5) -> Dict[str, Any]:
    """ê°„ë‹¨í•œ ê²€ìƒ‰ ë³´ê³ ì„œ ìƒì„±"""
    start_time = datetime.now()

    # ê²€ìƒ‰ ìˆ˜í–‰
    results = search_precedents_simple(query, top_n)

    search_time = (datetime.now() - start_time).total_seconds()

    if not results:
        return {
            "query": query,
            "total_results": 0,
            "search_time": search_time,
            "message": "ê´€ë ¨ëœ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "success": False
        }

    # ê¸°ë³¸ í†µê³„
    similarities = [r['similarity'] for r in results]
    courts = [r['court'] for r in results]

    # ë²•ì›ë³„ ë¶„í¬
    court_counts = {}
    for court in courts:
        court_counts[court] = court_counts.get(court, 0) + 1

    return {
        "query": query,
        "search_time": round(search_time, 3),
        "total_results": len(results),
        "success": True,
        "avg_similarity": round(np.mean(similarities), 3),
        "max_similarity": round(max(similarities), 3),
        "min_similarity": round(min(similarities), 3),
        "court_distribution": court_counts,
        "results": results[:top_n],
        "recommendation": _generate_simple_recommendation(similarities, court_counts)
    }

def _generate_simple_recommendation(similarities: List[float], court_counts: Dict[str, int]) -> str:
    """ê°„ë‹¨í•œ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
    avg_sim = np.mean(similarities)

    if avg_sim > 0.3:
        rec = "ë§¤ìš° ê´€ë ¨ì„± ë†’ì€ íŒë¡€ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
    elif avg_sim > 0.1:
        rec = "ì ì ˆí•œ ê´€ë ¨ì„±ì˜ íŒë¡€ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
    else:
        rec = "ìœ ì‚¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”. "

    if court_counts:
        main_court = max(court_counts.keys(), key=lambda k: court_counts[k])
        rec += f"ì£¼ë¡œ '{main_court}' ì‚¬ê±´ê³¼ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."

    return rec

# í¸ì˜ í•¨ìˆ˜ë“¤
def get_precedent_detail(case_id: str) -> Optional[Dict[str, Any]]:
    """ê°œë³„ íŒë¡€ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    # ëª¨ë¸ ë¡œë“œ í™•ì¸
    if not _model_loaded and not load_searcher_model_direct():
        logger.error("Model not loaded, cannot get precedent detail")
        return None

    try:
        # Case IDì—ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ (CASE_12345 -> 12345)
        if case_id.startswith('CASE_'):
            idx = int(case_id[5:])
        else:
            idx = int(case_id)

        # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
        if idx < 0 or idx >= len(_df):
            logger.error(f"Invalid case index: {idx}")
            return None

        # ë°ì´í„° ì¡°íšŒ
        row = _df.iloc[idx]

        title = str(row.get('title', 'Unknown Title'))
        court = str(row.get('courtname', 'Unknown Court'))
        date = format_court_date(str(row.get('kinda', 'Unknown Date')))

        # ì „ì²´ ë‚´ìš© (ìš”ì•½í•˜ì§€ ì•ŠìŒ)
        full_content = str(row.get('noncontent', ''))

        # ì¶”ê°€ ì •ë³´
        case_number = str(row.get('caseno', 'ì‚¬ê±´ë²ˆí˜¸ ë¯¸ìƒ'))
        plaintiff = str(row.get('plaintiff', 'ì›ê³  ì •ë³´ ë¯¸ìƒ'))
        defendant = str(row.get('defendant', 'í”¼ê³  ì •ë³´ ë¯¸ìƒ'))

        # ì¹´í…Œê³ ë¦¬
        category = get_friendly_category(title, full_content)

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        keywords = extract_keywords_from_content(full_content)

        detail = {
            'case_id': case_id,
            'index': idx,
            'title': title,
            'court': court,
            'date': date,
            'case_number': case_number,
            'plaintiff': plaintiff,
            'defendant': defendant,
            'category': category,
            'full_content': full_content,
            'keywords': keywords,
            'content_length': len(full_content),
            'summarized_title': summarize_case_title(title)
        }

        logger.info(f"Precedent detail retrieved successfully: {case_id}")
        return detail

    except Exception as e:
        logger.error(f"Failed to get precedent detail for {case_id}: {e}")
        return None

def extract_keywords_from_content(content: str, max_keywords: int = 10) -> List[str]:
    """íŒë¡€ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not content:
        return []

    # ì‚°ì¬ ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œë“¤
    important_terms = [
        'ì‚°ì—…ì¬í•´', 'ì—…ë¬´ìƒ', 'ì¬í•´', 'ë¶€ìƒ', 'ì‚¬ë§', 'ì§ˆë³‘',
        'ì•ˆì „ë³´ê±´', 'ì‘ì—…', 'ê·¼ë¡œì', 'ì‚¬ì—…ì£¼', 'ë³´ìƒê¸ˆ',
        'ì¹˜ë£Œë¹„', 'íœ´ì—…ê¸‰ì—¬', 'ì¥í•´ê¸‰ì—¬', 'ìœ ì¡±ê¸‰ì—¬', 'ì¥í•´ë“±ê¸‰',
        'ì˜ë£Œë¹„', 'ê°„ë³‘ë¹„', 'ì¶”ë½', 'ì ˆë‹¨', 'ê°ì „', 'í™”ìƒ',
        'ì¤‘ë…', 'ê³¨ì ˆ', 'ì—¼ì¢Œ', 'íƒ€ë°•ìƒ'
    ]

    found_keywords = []
    content_lower = content.lower()

    for term in important_terms:
        if term in content:
            found_keywords.append(term)
            if len(found_keywords) >= max_keywords:
                break

    return found_keywords

def quick_search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """ë¹ ë¥¸ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    return search_precedents_simple(query, top_k)

def test_search_service():
    """ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ” Simple Search Service Test")

    # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    if load_searcher_model_direct():
        print("âœ… Model loaded successfully")

        # í†µê³„ í™•ì¸
        stats = get_simple_search_stats()
        print(f"ğŸ“Š Total cases: {stats['total_cases']:,}")
        print(f"ğŸ“š Vocabulary size: {stats['vocabulary_size']:,}")

        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        test_query = "ì‘ì—… ì¤‘ ì†ê°€ë½ ë‹¤ì¹¨"
        print(f"\nğŸ” Test query: '{test_query}'")

        results = search_precedents_simple(test_query, 3)
        for i, result in enumerate(results, 1):
            print(f"{i}. [{result['similarity_pct']:.1f}%] {result['title'][:50]}...")
            print(f"   ë²•ì›: {result['court']}")

        # ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸
        report = generate_simple_report(test_query, 3)
        print(f"\nğŸ“‹ Report: {report['total_results']} results, "
              f"avg similarity: {report.get('avg_similarity', 'N/A')}")
        print(f"ğŸ’¡ Recommendation: {report.get('recommendation', 'N/A')}")

    else:
        print("âŒ Model loading failed")

if __name__ == "__main__":
    test_search_service()