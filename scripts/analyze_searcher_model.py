#!/usr/bin/env python3
"""
searcher_model.pkl íŒŒì¼ êµ¬ì¡° ìƒì„¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
AI íŒë¡€ ë¶„ì„ ì‹œìŠ¤í…œ í†µí•©ì„ ìœ„í•œ íŒŒì¼ ë‚´ìš© ë¶„ì„
"""

import pickle
import pandas as pd
import numpy as np
import sys
import os
import zlib
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def analyze_searcher_model():
    """searcher_model.pkl íŒŒì¼ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤."""

    pkl_path = project_root / "app" / "searcher_model.pkl"

    print("ğŸ” searcher_model.pkl íŒŒì¼ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")

    # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° í¬ê¸° í™•ì¸
    if not pkl_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
        return

    file_size_mb = pkl_path.stat().st_size / (1024 * 1024)
    print(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {pkl_path}")
    print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\n")

    try:
        # 2. ì••ì¶•ëœ pkl íŒŒì¼ ë¡œë“œ
        print("â³ ì••ì¶•ëœ pkl íŒŒì¼ì„ ë¡œë“œ ì¤‘...")
        with open(pkl_path, 'rb') as f:
            compressed_data = f.read()

        print("ğŸ“¦ zlib ì••ì¶• í•´ì œ ì¤‘...")
        decompressed_data = zlib.decompress(compressed_data)

        print("ğŸ”“ pickle ë°ì´í„° ë¡œë“œ ì¤‘...")
        # DataFrame ëª¨ë“ˆ ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°
        import pandas as pd
        sys.modules['DataFrame'] = pd.DataFrame

        model_data = pickle.loads(decompressed_data)

        print("âœ… pkl íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\n")

        # 3. ìµœìƒìœ„ êµ¬ì¡° ë¶„ì„
        print("ğŸ“‹ ìµœìƒìœ„ í‚¤ êµ¬ì¡°:")
        if isinstance(model_data, dict):
            for key in model_data.keys():
                print(f"  - {key}: {type(model_data[key])}")
        else:
            print(f"  íƒ€ì…: {type(model_data)}")
        print()

        # 4. ê° êµ¬ì„±ìš”ì†Œ ìƒì„¸ ë¶„ì„
        if isinstance(model_data, dict):

            # DataFrame ë¶„ì„
            if 'df' in model_data:
                analyze_dataframe(model_data['df'])

            # Vectorizer ë¶„ì„
            if 'vectorizer' in model_data:
                analyze_vectorizer(model_data['vectorizer'])

            # TF-IDF Matrix ë¶„ì„
            if 'tfidf_matrix' in model_data:
                analyze_tfidf_matrix(model_data['tfidf_matrix'])

            # Config ë¶„ì„
            if 'config' in model_data:
                analyze_config(model_data['config'])

            # ê¸°íƒ€ í‚¤ë“¤ ë¶„ì„
            for key, value in model_data.items():
                if key not in ['df', 'vectorizer', 'tfidf_matrix', 'config']:
                    print(f"ğŸ”§ ì¶”ê°€ êµ¬ì„±ìš”ì†Œ '{key}':")
                    print(f"  íƒ€ì…: {type(value)}")
                    if hasattr(value, 'shape'):
                        print(f"  í˜•íƒœ: {value.shape}")
                    print()

        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        analyze_memory_usage(model_data)

        # 6. ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        if isinstance(model_data, dict) and 'df' in model_data:
            analyze_sample_data(model_data['df'])

        print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def analyze_dataframe(df):
    """DataFrame êµ¬ì¡° ë¶„ì„"""
    print("ğŸ“Š DataFrame ë¶„ì„:")
    print(f"  í–‰ ìˆ˜: {len(df):,}ê°œ")
    print(f"  ì—´ ìˆ˜: {len(df.columns)}ê°œ")
    print(f"  ì»¬ëŸ¼: {list(df.columns)}")
    print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

    # ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ í™•ì¸
    print("\n  ğŸ“‹ ì»¬ëŸ¼ ìƒì„¸ ì •ë³´:")
    for col in df.columns:
        print(f"    {col}:")
        print(f"      íƒ€ì…: {df[col].dtype}")
        print(f"      ê²°ì¸¡ê°’: {df[col].isnull().sum()}ê°œ")
        if not df[col].empty:
            sample_val = str(df[col].iloc[0])
            if len(sample_val) > 100:
                sample_val = sample_val[:100] + "..."
            print(f"      ìƒ˜í”Œ: {sample_val}")
    print()

def analyze_vectorizer(vectorizer):
    """TF-IDF Vectorizer ë¶„ì„"""
    print("ğŸ”¤ TF-IDF Vectorizer ë¶„ì„:")
    print(f"  íƒ€ì…: {type(vectorizer)}")

    if hasattr(vectorizer, 'vocabulary_'):
        vocab_size = len(vectorizer.vocabulary_)
        print(f"  ì–´íœ˜ í¬ê¸°: {vocab_size:,}ê°œ")

    # Vectorizer ì„¤ì • í™•ì¸
    if hasattr(vectorizer, 'get_params'):
        params = vectorizer.get_params()
        print(f"  ì£¼ìš” ì„¤ì •:")
        for key, value in params.items():
            if key in ['max_features', 'ngram_range', 'min_df', 'max_df', 'sublinear_tf', 'use_idf']:
                print(f"    {key}: {value}")

    # ìƒìœ„ ë¹ˆë„ ë‹¨ì–´ë“¤ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
    if hasattr(vectorizer, 'vocabulary_'):
        vocab_items = list(vectorizer.vocabulary_.items())
        if vocab_items:
            print(f"  ì–´íœ˜ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
            for word, idx in sorted(vocab_items, key=lambda x: x[1])[:10]:
                print(f"    '{word}' -> {idx}")
    print()

def analyze_tfidf_matrix(tfidf_matrix):
    """TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ë¶„ì„"""
    print("ğŸ“ˆ TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ë¶„ì„:")
    print(f"  íƒ€ì…: {type(tfidf_matrix)}")
    print(f"  í˜•íƒœ: {tfidf_matrix.shape}")
    print(f"  ë°ì´í„° íƒ€ì…: {tfidf_matrix.dtype}")

    if hasattr(tfidf_matrix, 'nnz'):
        total_elements = np.prod(tfidf_matrix.shape)
        sparsity = (total_elements - tfidf_matrix.nnz) / total_elements * 100
        print(f"  í¬ì†Œì„±: {sparsity:.2f}% (0ì´ ì•„ë‹Œ ê°’: {tfidf_matrix.nnz:,}ê°œ)")

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
    if hasattr(tfidf_matrix, 'data'):
        memory_mb = (tfidf_matrix.data.nbytes + tfidf_matrix.indices.nbytes +
                    tfidf_matrix.indptr.nbytes) / 1024 / 1024
        print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.2f} MB")
    print()

def analyze_config(config):
    """Config ì„¤ì • ë¶„ì„"""
    print("âš™ï¸ Config ì„¤ì • ë¶„ì„:")
    print(f"  íƒ€ì…: {type(config)}")

    if isinstance(config, dict):
        for key, value in config.items():
            print(f"  {key}:")
            if isinstance(value, (list, tuple)):
                print(f"    ê°œìˆ˜: {len(value)}ê°œ")
                if len(value) <= 10:
                    print(f"    ë‚´ìš©: {value}")
                else:
                    print(f"    ìƒ˜í”Œ: {value[:5]}... (ì´ {len(value)}ê°œ)")
            else:
                print(f"    ê°’: {value}")
    print()

def analyze_memory_usage(model_data):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
    print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„:")

    if isinstance(model_data, dict):
        total_size = 0
        for key, value in model_data.items():
            size_mb = estimate_object_size(value)
            total_size += size_mb
            print(f"  {key}: {size_mb:.2f} MB")

        print(f"  ğŸ“Š ì´ ì˜ˆìƒ ë©”ëª¨ë¦¬: {total_size:.2f} MB")
    print()

def estimate_object_size(obj):
    """ê°ì²´ì˜ ëŒ€ëµì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB ë‹¨ìœ„)"""
    if isinstance(obj, pd.DataFrame):
        return obj.memory_usage(deep=True).sum() / 1024 / 1024
    elif hasattr(obj, 'data') and hasattr(obj, 'indices'):  # í¬ì†Œ í–‰ë ¬
        return (obj.data.nbytes + obj.indices.nbytes + obj.indptr.nbytes) / 1024 / 1024
    elif isinstance(obj, (list, dict, str)):
        return sys.getsizeof(obj) / 1024 / 1024
    else:
        return sys.getsizeof(obj) / 1024 / 1024

def analyze_sample_data(df):
    """ìƒ˜í”Œ ë°ì´í„° í™•ì¸"""
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„° (ì²« 3ê±´):")

    if not df.empty:
        for i in range(min(3, len(df))):
            print(f"\n  ğŸ“„ íŒë¡€ {i+1}:")
            row = df.iloc[i]
            for col in df.columns:
                value = str(row[col])
                if len(value) > 150:
                    value = value[:150] + "..."
                print(f"    {col}: {value}")
    print()

if __name__ == "__main__":
    analyze_searcher_model()