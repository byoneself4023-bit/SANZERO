"""
AI íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤
RAG ê¸°ë°˜ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ë° LLM ë¶„ì„ ì„œë¹„ìŠ¤
"""

import json
import asyncio
import logging
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime, timezone
from app.utils.database import supabase
from app.utils.security import security
from app.utils.config import settings
from app.utils.api_monitor import api_monitor

# ì¡°ê±´ë¶€ import - ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ë„ ì„œë¹„ìŠ¤ ë™ì‘
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None
    NUMPY_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SentenceTransformer = None
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    openai = None
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    anthropic = None
    ANTHROPIC_AVAILABLE = False


class AnalysisService:
    """AI íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤"""

    def __init__(self):
        # ë¡œê±° ì´ˆê¸°í™”
        self.logger = logging.getLogger(__name__)

        # SBERT ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ ìµœì í™”)
        self.embedding_model = None
        self.openai_client = None
        self.anthropic_client = None
        self.fallback_mode = False
        self._initialize_models()

    def _initialize_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # SBERT ëª¨ë¸ ë¡œë“œ ì‹œë„
            if SENTENCE_TRANSFORMERS_AVAILABLE and SentenceTransformer:
                try:
                    self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                    self.logger.info("SBERT embedding model loaded successfully")
                except Exception as e:
                    self.logger.warning(f"Failed to load SBERT model: {e}, using fallback mode")
                    self.fallback_mode = True
            else:
                self.logger.warning("sentence-transformers not available, using fallback mode")
                self.fallback_mode = True

            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if OPENAI_AVAILABLE and openai and hasattr(settings, 'OPENAI_API_KEY') and settings.OPENAI_API_KEY:
                try:
                    self.openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
                    self.logger.info("OpenAI client initialized")
                except Exception as e:
                    self.logger.warning(f"Failed to initialize OpenAI client: {e}")
            else:
                self.logger.warning("OpenAI library not available or API key not set")

            # Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if ANTHROPIC_AVAILABLE and anthropic and hasattr(settings, 'ANTHROPIC_API_KEY') and settings.ANTHROPIC_API_KEY:
                try:
                    self.anthropic_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
                    self.logger.info("Anthropic client initialized")
                except Exception as e:
                    self.logger.warning(f"Failed to initialize Anthropic client: {e}")
            else:
                self.logger.warning("Anthropic library not available or API key not set")

        except Exception as e:
            self.logger.error(f"Failed to initialize AI models: {e}")
            self.fallback_mode = True
            # ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì„œë¹„ìŠ¤ëŠ” ë™ì‘í•˜ë„ë¡ í•¨

    async def generate_embedding(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_text = security.sanitize_text(text)
            if not cleaned_text.strip():
                return None

            # Fallback ëª¨ë“œ ë˜ëŠ” ëª¨ë¸ ì—†ìŒ
            if self.fallback_mode or not self.embedding_model:
                self.logger.info("Using fallback embedding generation")
                return self._generate_fallback_embedding(cleaned_text)

            # ì‹¤ì œ SBERT ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬)
            embedding = await asyncio.to_thread(
                lambda: self.embedding_model.encode(cleaned_text)
            )
            return embedding.tolist()

        except Exception as e:
            self.logger.error(f"Failed to generate embedding: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ fallback ì‚¬ìš©
            return self._generate_fallback_embedding(cleaned_text)

    def _generate_fallback_embedding(self, text: str) -> List[float]:
        """Fallback ì„ë² ë”© ìƒì„± (ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)"""
        import hashlib
        import math

        # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŠ¹ì§• ë²¡í„° ìƒì„± (384ì°¨ì›)
        features = []

        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ íŠ¹ì§•
        features.append(len(text) / 1000.0)  # ì •ê·œí™”

        # 2. ë¬¸ì ë¹ˆë„ íŠ¹ì§•
        char_counts = {}
        for char in text.lower():
            char_counts[char] = char_counts.get(char, 0) + 1

        # ì£¼ìš” ë¬¸ìë“¤ì˜ ë¹ˆë„
        common_chars = 'abcdefghijklmnopqrstuvwxyzê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜'
        char_limit = min(50, len(common_chars))  # ì•ˆì „í•œ ê¸¸ì´ ì œí•œ
        for char in common_chars[:char_limit]:  # ì‹¤ì œ ë¬¸ì ê°œìˆ˜ë§Œí¼ (40ê°œ)
            features.append(char_counts.get(char, 0) / len(text) if text else 0)

        # 3. í•´ì‹œ ê¸°ë°˜ íŠ¹ì§•
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()

        for i in range(32):  # 32ë°”ì´íŠ¸
            features.append((hash_bytes[i] % 256) / 255.0)

        # 4. í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¹ì§• (ì‚°ì¬ ê´€ë ¨)
        keywords = [
            'ì‚°ì—…ì¬í•´', 'ì‚°ì¬', 'ì‚¬ê³ ', 'ë¶€ìƒ', 'ì ˆë‹¨', 'ê³¨ì ˆ', 'í™”ìƒ', 'íƒ€ë°•ìƒ',
            'ì œì¡°ì—…', 'ê±´ì„¤ì—…', 'ì„œë¹„ìŠ¤ì—…', 'ê¸°ê³„', 'ì•ˆì „', 'ë³´ìƒê¸ˆ', 'ìŠ¹ì¸', 'ê±°ë¶€',
            'ë³‘ì›', 'ì¹˜ë£Œ', 'ìˆ˜ìˆ ', 'ì¬í™œ', 'ì¥í•´', 'ë“±ê¸‰', 'ë…¸ë¬´ì‚¬', 'ìƒë‹´'
        ]

        for keyword in keywords:
            features.append(1.0 if keyword in text else 0.0)

        # 5. ë‚˜ë¨¸ì§€ ì°¨ì›ì„ 0ìœ¼ë¡œ íŒ¨ë”©í•˜ê±°ë‚˜ ë°˜ë³µìœ¼ë¡œ ì±„ì›€
        target_dim = 1536  # OpenAI text-embedding-ada-002 ì°¨ì›ê³¼ ì¼ì¹˜
        current_len = len(features)

        # ê°„ë‹¨í•œ íŒ¨ë”©
        for i in range(target_dim - current_len):
            features.append(0.1 * (i % 10))

        # ì •í™•íˆ 1536ì°¨ì›ìœ¼ë¡œ ìë¥´ê¸°
        return features[:target_dim]

    async def search_similar_precedents(
        self,
        query_text: str,
        similarity_threshold: float = 0.7,
        max_results: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            query_embedding = await self.generate_embedding(query_text)
            if not query_embedding:
                self.logger.error("Failed to generate query embedding")
                return []

            # Supabaseì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            query = supabase.table("precedents").select("""
                id, case_number, title, summary, court_name, case_date,
                outcome, compensation_amount, injury_type, industry_code,
                keywords
            """).eq("is_active", True)

            # í•„í„° ì ìš©
            if filters:
                if filters.get("injury_type"):
                    query = query.eq("injury_type", filters["injury_type"])
                if filters.get("industry_code"):
                    query = query.eq("industry_code", filters["industry_code"])
                if filters.get("outcome"):
                    query = query.eq("outcome", filters["outcome"])

            # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰
            response = await asyncio.to_thread(
                lambda: query.limit(max_results * 2).execute()  # í•„í„°ë§ ê³ ë ¤í•´ ë” ë§ì´ ê°€ì ¸ì˜´
            )

            if not response.data:
                return []

            # ì‹¤ì œ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°
            similar_precedents = []
            for precedent in response.data:
                precedent_embedding = precedent.get("embedding")

                if precedent_embedding and query_embedding:
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity_score = self._calculate_cosine_similarity(
                        query_embedding, precedent_embedding
                    )
                else:
                    # ì„ë² ë”©ì´ ì—†ëŠ” ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„
                    similarity_score = self._calculate_text_similarity(
                        query_text, precedent.get("title", "") + " " + precedent.get("summary", "")
                    )

                similar_precedents.append({
                    "precedent": precedent,
                    "similarity_score": round(similarity_score, 3),
                    "matching_factors": {
                        "injury_type_match": filters and filters.get("injury_type") == precedent.get("injury_type"),
                        "industry_match": filters and filters.get("industry_code") == precedent.get("industry_code"),
                        "embedding_match": precedent_embedding is not None
                    }
                })

            # ìœ ì‚¬ë„ë¡œ ì •ë ¬
            similar_precedents.sort(key=lambda x: x["similarity_score"], reverse=True)

            # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            filtered_precedents = [
                p for p in similar_precedents
                if p["similarity_score"] >= similarity_threshold
            ]

            return filtered_precedents[:max_results]

        except Exception as e:
            self.logger.error(f"Failed to search similar precedents: {e}")
            return []

    async def analyze_precedents_with_llm(
        self,
        user_case: Dict[str, Any],
        similar_precedents: List[Dict[str, Any]],
        analysis_type: str = "comprehensive"
    ) -> Optional[Dict[str, Any]]:
        """LLMì„ í†µí•œ íŒë¡€ ë¶„ì„"""
        try:
            # ì‚¬ìš©ì ì‚¬ê±´ ìš”ì•½
            case_summary = self._create_case_summary(user_case)

            # ìœ ì‚¬ íŒë¡€ ìš”ì•½
            precedents_summary = self._create_precedents_summary(similar_precedents)

            # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_analysis_prompt(case_summary, precedents_summary, analysis_type)

            # LLM ë¶„ì„ ì‹¤í–‰
            analysis_result = await self._call_llm_analysis(prompt)

            if not analysis_result:
                return None

            # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°í™”
            structured_result = {
                "analysis_summary": analysis_result.get("summary", ""),
                "success_probability": analysis_result.get("success_probability", 50.0),
                "key_factors": analysis_result.get("key_factors", []),
                "similar_precedents_analysis": analysis_result.get("precedents_analysis", []),
                "recommended_actions": analysis_result.get("recommended_actions", []),
                "legal_reasoning": analysis_result.get("legal_reasoning", ""),
                "risk_assessment": analysis_result.get("risk_assessment", {}),
                "confidence_score": analysis_result.get("confidence", 75.0),
                "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
                "model_version": "gpt-4-turbo"  # ë˜ëŠ” ì‚¬ìš©ëœ ëª¨ë¸ëª…
            }

            return structured_result

        except Exception as e:
            self.logger.error(f"Failed to analyze precedents with LLM: {e}")
            return None

    def _create_case_summary(self, user_case: Dict[str, Any]) -> str:
        """ì‚¬ìš©ì ì‚¬ê±´ ìš”ì•½ ìƒì„±"""
        return f"""
        ì‚¬ê³  ë‚ ì§œ: {user_case.get('incident_date', 'N/A')}
        ì‚¬ê³  ì¥ì†Œ: {user_case.get('incident_location', 'N/A')}
        ë¶€ìƒ ìœ í˜•: {user_case.get('injury_type', 'N/A')}
        ì‚¬ê³  ê²½ìœ„: {user_case.get('incident_description', 'N/A')}
        ì‹¬ê°ë„: {user_case.get('severity_level', 'N/A')}
        ì—…ì¢…: {user_case.get('industry_code', 'N/A')}
        """

    def _create_precedents_summary(self, precedents: List[Dict[str, Any]]) -> str:
        """ìœ ì‚¬ íŒë¡€ ìš”ì•½ ìƒì„± (í† í° ì œí•œ ê³ ë ¤)"""
        if not precedents:
            return "ìœ ì‚¬ íŒë¡€ ì—†ìŒ"

        summary_parts = []
        for i, item in enumerate(precedents[:2], 1):  # ìƒìœ„ 2ê°œë§Œìœ¼ë¡œ ë” ì¶•ì†Œ (ë¹„ìš© ì ˆì•½)
            precedent = item["precedent"]
            similarity = item["similarity_score"]

            # ìš”ì•½ì„ 30ìë¡œ ë” ì¶•ì†Œ (ë¹„ìš© ì ˆì•½)
            short_summary = precedent.get('summary', 'N/A')[:30] + "..."

            summary_parts.append(f"íŒë¡€{i}({similarity:.1f}): {precedent.get('outcome', 'N/A')}, {precedent.get('compensation_amount', 0)//10000}ë§Œì›")

        return "\n".join(summary_parts)

    def _create_analysis_prompt(
        self,
        case_summary: str,
        precedents_summary: str,
        analysis_type: str
    ) -> str:
        """LLM ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í† í° ìµœì í™”)"""
        return f"""ì‚¬ê±´: {case_summary}
íŒë¡€: {precedents_summary}

JSON ì‘ë‹µ:
{{
    "summary": "ë¶„ì„ìš”ì•½(1ë¬¸ì¥)",
    "success_probability": 75.5,
    "key_factors": {{"positive": ["ìš”ì¸1"], "negative": ["ìš”ì¸1"]}},
    "recommended_actions": ["ì¡°ì¹˜1", "ì¡°ì¹˜2"],
    "confidence": 85.0
}}"""

    async def _call_llm_analysis(self, prompt: str) -> Optional[Dict[str, Any]]:
        """LLM API í˜¸ì¶œ"""
        try:
            # OpenAI API ì‚¬ìš© (ìš°ì„ ìˆœìœ„)
            if self.openai_client:
                response = await self._call_openai(prompt)
                if response:
                    return response

            # Anthropic API ì‚¬ìš© (ëŒ€ì•ˆ)
            if self.anthropic_client:
                response = await self._call_anthropic(prompt)
                if response:
                    return response

            # Fallback ë¶„ì„ (LLM ì—†ì´)
            self.logger.warning("No LLM client available, using fallback analysis")
            return self._generate_fallback_analysis(prompt)

        except Exception as e:
            self.logger.error(f"Failed to call LLM: {e}")
            # ì—ëŸ¬ ì‹œì—ë„ fallback ì‚¬ìš©
            return self._generate_fallback_analysis(prompt)

    def _generate_fallback_analysis(self, prompt: str) -> Dict[str, Any]:
        """Fallback ë¶„ì„ ìƒì„± (LLM API ì—†ì´)"""
        # í”„ë¡¬í”„íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
        prompt_lower = prompt.lower()

        # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ë¦¬ë„ ê³„ì‚°
        positive_keywords = ['ì—…ë¬´ì¤‘', 'ì‚¬ê³ ', 'ë¶€ìƒ', 'ì•ˆì „ì¥ì¹˜', 'ë¶€ì¡±', 'ì œì¡°ì—…', 'ê±´ì„¤ì—…', 'ì ˆë‹¨', 'ê³¨ì ˆ']
        negative_keywords = ['ìŒì£¼', 'ê³ ì˜', 'ê°œì¸ì ', 'ì§ˆë³‘']

        positive_score = sum(10 for keyword in positive_keywords if keyword in prompt_lower)
        negative_score = sum(15 for keyword in negative_keywords if keyword in prompt_lower)

        base_probability = 65.0  # ê¸°ë³¸ ìœ ë¦¬ë„
        final_probability = max(20.0, min(95.0, base_probability + positive_score - negative_score))

        # ë¶€ìƒ ìœ í˜•ë³„ ì¡°ì •
        if any(injury in prompt_lower for injury in ['ì ˆë‹¨', 'ê³¨ì ˆ', 'í™”ìƒ']):
            final_probability += 10.0
        elif any(injury in prompt_lower for injury in ['íƒ€ë°•ìƒ', 'ì—¼ì¢Œ']):
            final_probability += 5.0

        # ì—…ì¢…ë³„ ì¡°ì •
        if any(industry in prompt_lower for industry in ['ì œì¡°ì—…', 'ê±´ì„¤ì—…']):
            final_probability += 5.0

        final_probability = max(15.0, min(95.0, final_probability))

        return {
            "summary": f"ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼, ì‚¬ì•ˆì˜ ìœ ë¦¬ë„ëŠ” {final_probability:.1f}%ì…ë‹ˆë‹¤. ì—…ë¬´ ê´€ë ¨ì„±ê³¼ ì‚¬ê³  ê²½ìœ„ë¥¼ ê³ ë ¤í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
            "favorability_score": final_probability,
            "key_factors": {
                "positive": [
                    "ì—…ë¬´ ì¤‘ ë°œìƒí•œ ì‚¬ê³ ",
                    "ëª…í™•í•œ ì‚¬ê³  ê²½ìœ„",
                    "ì¦‰ì‹œ ë³‘ì› ì¹˜ë£Œ"
                ] if positive_score > 0 else [],
                "negative": [
                    "ì¶”ê°€ ê²€í†  í•„ìš” ì‚¬í•­ ì¡´ì¬"
                ] if negative_score > 0 else []
            },
            "recommended_actions": [
                "ì˜ë£Œì§„ë‹¨ì„œ ë° ì†Œê²¬ì„œ ì¤€ë¹„",
                "ì‚¬ê³  ê²½ìœ„ì„œ ìƒì„¸ ì‘ì„±",
                "ëª©ê²©ì ì§„ìˆ ì„œ í™•ë³´",
                "ë…¸ë¬´ì‚¬ ìƒë‹´ì„ í†µí•œ ì „ë¬¸ ê²€í† "
            ],
            "risk_assessment": {
                "level": "ì¤‘ê°„" if final_probability < 70 else "ë‚®ìŒ",
                "factors": [
                    "ì—…ë¬´ ê´€ë ¨ì„± ì…ì¦ í•„ìš”",
                    "ì˜ë£Œ ê¸°ë¡ ì¶©ì‹¤ì„±",
                    "ì‚¬ê³  ê²½ìœ„ì˜ ëª…í™•ì„±"
                ]
            },
            "confidence": 75.0,
            "model_note": "í˜„ì¬ AI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ ë…¸ë¬´ì‚¬ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        }

    async def _call_openai(self, prompt: str) -> Optional[Dict[str, Any]]:
        """OpenAI API í˜¸ì¶œ (í† í° ìµœì í™”)"""
        try:
            # Rate limiting í™•ì¸
            if not await api_monitor.check_rate_limit("analysis"):
                self.logger.warning("Analysis service rate limit exceeded")
                return None

            response = await asyncio.to_thread(
                lambda: self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",  # ë” íš¨ìœ¨ì ì¸ ëª¨ë¸ ì‚¬ìš©
                    messages=[
                        {"role": "system", "content": "ì‚°ì¬ì „ë¬¸ê°€"},  # í”„ë¡¬í”„íŠ¸ ë‹¨ì¶•
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=500  # í† í° ìˆ˜ ëŒ€í­ ê°ì†Œ (ë¹„ìš© ì ˆì•½)
                )
            )

            # API ì‚¬ìš©ëŸ‰ ì¶”ì 
            usage = response.usage
            await api_monitor.track_usage(
                service="analysis",
                provider="openai",
                model="gpt-4o-mini",
                input_tokens=usage.prompt_tokens,
                output_tokens=usage.completion_tokens
            )

            content = response.choices[0].message.content

            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                # JSONì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                return {"summary": content, "success_probability": 50.0}

        except Exception as e:
            self.logger.error(f"OpenAI API call failed: {e}")
            return None

    async def _call_anthropic(self, prompt: str) -> Optional[Dict[str, Any]]:
        """Anthropic Claude API í˜¸ì¶œ (í† í° ìµœì í™”)"""
        try:
            # Rate limiting í™•ì¸
            if not await api_monitor.check_rate_limit("analysis"):
                self.logger.warning("Analysis service rate limit exceeded")
                return None

            response = await asyncio.to_thread(
                lambda: self.anthropic_client.messages.create(
                    model="claude-3-haiku-20240307",  # ë” íš¨ìœ¨ì ì¸ ëª¨ë¸ ì‚¬ìš©
                    max_tokens=500,  # í† í° ìˆ˜ ëŒ€í­ ê°ì†Œ (ë¹„ìš© ì ˆì•½)
                    temperature=0.3,
                    system="ì‚°ì¬ì „ë¬¸ê°€",  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‹¨ì¶•
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
            )

            # API ì‚¬ìš©ëŸ‰ ì¶”ì 
            usage = response.usage
            await api_monitor.track_usage(
                service="analysis",
                provider="anthropic",
                model="claude-3-haiku-20240307",
                input_tokens=usage.input_tokens,
                output_tokens=usage.output_tokens
            )

            content = response.content[0].text

            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                return {"summary": content, "success_probability": 50.0}

        except Exception as e:
            self.logger.error(f"Anthropic API call failed: {e}")
            return None

    async def create_analysis_request(
        self,
        user_id: str,
        query_text: str,
        case_description: str,
        application_id: Optional[str] = None,
        industry_type: Optional[str] = None,
        injury_type: Optional[str] = None,
        accident_circumstances: Optional[str] = None
    ) -> Optional[str]:
        """ë¶„ì„ ìš”ì²­ ìƒì„±"""
        try:
            # XSS ë°©ì–´
            query_text = security.sanitize_text(query_text)
            case_description = security.sanitize_text(case_description)
            if accident_circumstances:
                accident_circumstances = security.sanitize_text(accident_circumstances)

            request_data = {
                "user_id": user_id,
                "application_id": application_id,
                "query_text": query_text,
                "analysis_type": "precedent_search",
                "status": "pending",
                "is_active": True,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "updated_at": datetime.now(timezone.utc).isoformat()
            }

            response = await asyncio.to_thread(
                lambda: supabase.table("analysis_requests").insert(request_data).execute()
            )

            if response.data:
                request_id = response.data[0]["id"]
                self.logger.info(f"Analysis request created: {request_id}")

                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ìˆ˜í–‰
                asyncio.create_task(self._process_analysis_request(request_id, {
                    "case_description": case_description,
                    "industry_type": industry_type,
                    "injury_type": injury_type,
                    "accident_circumstances": accident_circumstances,
                    "query_text": query_text
                }))

                return request_id

            return None

        except Exception as e:
            self.logger.error(f"Failed to create analysis request: {e}")
            return None

    async def _process_analysis_request(self, request_id: str, case_data: Dict[str, Any]):
        """ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ)"""
        try:
            start_time = datetime.now()

            # ìƒíƒœë¥¼ processingìœ¼ë¡œ ë³€ê²½
            await asyncio.to_thread(
                lambda: supabase.table("analysis_requests")
                .update({"status": "processing", "updated_at": datetime.now(timezone.utc).isoformat()})
                .eq("id", request_id).execute()
            )

            # 1. ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
            similar_precedents = await self.search_similar_precedents(
                query_text=case_data["query_text"],
                similarity_threshold=0.6,
                max_results=10,
                filters={
                    "injury_type": case_data.get("injury_type"),
                    "industry_code": case_data.get("industry_type")
                }
            )

            # 2. LLM ë¶„ì„
            analysis_result = await self.analyze_precedents_with_llm(
                user_case=case_data,
                similar_precedents=similar_precedents,
                analysis_type="comprehensive"
            )

            # 3. ê²°ê³¼ ì €ì¥
            processing_time = int((datetime.now() - start_time).total_seconds() * 1000)

            result_data = {
                "similar_precedents": similar_precedents,
                "analysis_result": analysis_result,
                "processing_time_ms": processing_time
            }

            await asyncio.to_thread(
                lambda: supabase.table("analysis_requests")
                .update({
                    "status": "completed",
                    "result": result_data,
                    "processing_time_ms": processing_time,
                    "updated_at": datetime.now(timezone.utc).isoformat()
                })
                .eq("id", request_id).execute()
            )

            self.logger.info(f"Analysis request {request_id} completed in {processing_time}ms")

        except Exception as e:
            self.logger.error(f"Failed to process analysis request {request_id}: {e}")

            # ì—ëŸ¬ ìƒíƒœë¡œ ë³€ê²½
            await asyncio.to_thread(
                lambda: supabase.table("analysis_requests")
                .update({
                    "status": "failed",
                    "error_message": str(e),
                    "updated_at": datetime.now(timezone.utc).isoformat()
                })
                .eq("id", request_id).execute()
            )

    async def get_analysis_result(self, request_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
        try:
            response = await asyncio.to_thread(
                lambda: supabase.table("analysis_requests")
                .select("*")
                .eq("id", request_id)
                .eq("user_id", user_id)
                .eq("is_active", True)
                .single()
                .execute()
            )

            if response.data:
                result = response.data.copy()

                # ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜ (í…œí”Œë¦¿ì—ì„œ strftime ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
                try:
                    if result.get("created_at"):
                        result["created_at"] = datetime.fromisoformat(result["created_at"].replace('Z', '+00:00'))
                    if result.get("updated_at"):
                        result["updated_at"] = datetime.fromisoformat(result["updated_at"].replace('Z', '+00:00'))
                except (ValueError, AttributeError) as e:
                    self.logger.warning(f"Failed to parse datetime fields: {e}")
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€

                return result

            return None

        except Exception as e:
            self.logger.error(f"Failed to get analysis result: {e}")
            return None

    async def get_user_analysis_history(
        self,
        user_id: str,
        limit: int = 20,
        status_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì ë¶„ì„ ìš”ì²­ ë‚´ì—­ ì¡°íšŒ"""
        try:
            query = supabase.table("analysis_requests").select("""
                id, query_text, analysis_type, status,
                processing_time_ms, created_at, updated_at
            """).eq("user_id", user_id).eq("is_active", True)

            if status_filter:
                query = query.eq("status", status_filter)

            response = await asyncio.to_thread(
                lambda: query.order("created_at", desc=True).limit(limit).execute()
            )

            return response.data or []

        except Exception as e:
            self.logger.error(f"Failed to get user analysis history: {e}")
            return []

    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if NUMPY_AVAILABLE and np:
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                arr1 = np.array(vec1)
                arr2 = np.array(vec2)

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³µì‹: dot(a, b) / (norm(a) * norm(b))
                dot_product = np.dot(arr1, arr2)
                norm1 = np.linalg.norm(arr1)
                norm2 = np.linalg.norm(arr2)

                if norm1 == 0 or norm2 == 0:
                    return 0.0

                similarity = dot_product / (norm1 * norm2)
                return max(0.0, min(1.0, similarity))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
            else:
                # Pure Python fallback
                return self._calculate_cosine_similarity_fallback(vec1, vec2)

        except Exception as e:
            self.logger.error(f"Failed to calculate cosine similarity: {e}")
            return 0.0

    def _calculate_cosine_similarity_fallback(self, vec1: List[float], vec2: List[float]) -> float:
        """ìˆœìˆ˜ Pythonìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (numpy ì—†ì´)"""
        try:
            import math

            # ë‚´ì  ê³„ì‚°
            dot_product = sum(a * b for a, b in zip(vec1, vec2))

            # ë…¸ë¦„ ê³„ì‚°
            norm1 = math.sqrt(sum(a * a for a in vec1))
            norm2 = math.sqrt(sum(b * b for b in vec2))

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            return max(0.0, min(1.0, similarity))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘

        except Exception as e:
            self.logger.error(f"Failed to calculate cosine similarity fallback: {e}")
            return 0.0

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (ë²¡í„° ì„ë² ë”©ì´ ì—†ëŠ” ê²½ìš°)"""
        try:
            if not text1 or not text2:
                return 0.0

            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())

            if not words1 or not words2:
                return 0.0

            # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))

            return intersection / union if union > 0 else 0.0

        except Exception as e:
            self.logger.error(f"Failed to calculate text similarity: {e}")
            return 0.0

    async def summarize_precedent_content(self, case_id: str, content: str, title: str = "") -> Dict[str, Any]:
        """
        ë³µì¡í•œ íŒê²°ë¬¸ì„ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•˜ëŠ” ê¸°ëŠ¥

        Args:
            case_id: íŒë¡€ ID
            content: íŒê²°ë¬¸ ì „ì²´ ë‚´ìš©
            title: íŒë¡€ ì œëª© (ì„ íƒì‚¬í•­)

        Returns:
            Dict containing:
                - summary: ì‰¬ìš´ ì–¸ì–´ë¡œ ì‘ì„±ëœ ìš”ì•½
                - key_points: í•µì‹¬ í¬ì¸íŠ¸ë“¤
                - outcome: íŒê²° ê²°ê³¼ ìš”ì•½
                - significance: ì´ íŒë¡€ì˜ ì˜ë¯¸
        """
        try:
            self.logger.info(f"Starting precedent content summarization for case {case_id}")

            # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ ë¶€ë¶„ë§Œ ì‚¬ìš© (í† í° ì œí•œ ê³ ë ¤)
            max_content_length = 2000
            truncated_content = content[:max_content_length] if len(content) > max_content_length else content

            # ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            summary_prompt = f"""
ë‹¤ìŒ íŒë¡€ë¥¼ ì¼ë°˜ì¸ì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½í•´ì£¼ì„¸ìš”.

ã€íŒë¡€ ì •ë³´ã€‘
ì œëª©: {title if title else 'ì œëª© ì—†ìŒ'}
ë‚´ìš©: {truncated_content}

ã€ìš”ì•½ ê°€ì´ë“œã€‘
1. ì „ë¬¸ ë²•ë¥  ìš©ì–´ë¥¼ ì¼ìƒ ì–¸ì–´ë¡œ ë°”ê¿”ì„œ ì„¤ëª…
2. í•µì‹¬ ìŸì ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì •ë¦¬
3. íŒê²° ê²°ê³¼ì™€ ê·¸ ì´ìœ ë¥¼ ì•Œê¸° ì‰½ê²Œ ì„¤ëª…
4. ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

**ğŸ›ï¸ ì‚¬ê±´ ê°œìš”**
(ë¬´ì—‡ì— ëŒ€í•œ ë¶„ìŸì¸ì§€ ê°„ë‹¨íˆ)

**ğŸ¯ í•µì‹¬ ìŸì **
(ë²•ì›ì—ì„œ íŒë‹¨í•´ì•¼ í•  ì£¼ìš” ë¬¸ì œ)

**âš–ï¸ íŒê²° ê²°ê³¼**
(ë²•ì›ì˜ ê²°ë¡ ê³¼ ê·¸ ì´ìœ )

**ğŸ’¡ ì´ íŒë¡€ì˜ ì˜ë¯¸**
(ì´í›„ ìœ ì‚¬í•œ ì‚¬ê±´ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€)

**ğŸ“ í•œì¤„ ìš”ì•½**
(ì´ íŒë¡€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ë©´)
"""

            # AI ìš”ì•½ ì‹¤í–‰
            if OPENAI_AVAILABLE and self.openai_client:
                summary_result = await self._call_openai_for_summary(summary_prompt)
            elif ANTHROPIC_AVAILABLE and self.anthropic_client:
                summary_result = await self._call_anthropic_for_summary(summary_prompt)
            else:
                # Fallback: ê·œì¹™ ê¸°ë°˜ ê°„ë‹¨ ìš”ì•½
                summary_result = self._create_simple_summary(truncated_content, title)

            # ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "case_id": case_id,
                "summary": summary_result.get("summary", "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "key_points": summary_result.get("key_points", []),
                "outcome": summary_result.get("outcome", "íŒê²° ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "significance": summary_result.get("significance", "ì´ íŒë¡€ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
                "one_line_summary": summary_result.get("one_line_summary", "ê°„ë‹¨ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "content_length": len(content),
                "truncated": len(content) > max_content_length
            }

            self.logger.info(f"Successfully summarized precedent {case_id}")
            return result

        except Exception as e:
            self.logger.error(f"Error summarizing precedent content for {case_id}: {e}")
            return {
                "success": False,
                "case_id": case_id,
                "error": f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "summary": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì´ íŒë¡€ë¥¼ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "key_points": [],
                "outcome": "íŒê²° ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "significance": "íŒë¡€ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "one_line_summary": "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    async def _call_openai_for_summary(self, prompt: str) -> Dict[str, Any]:
        """OpenAI APIë¥¼ ì‚¬ìš©í•œ íŒë¡€ ìš”ì•½"""
        try:
            response = await asyncio.to_thread(
                lambda: self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì´ë©´ì„œ ë™ì‹œì— ì¼ë°˜ì¸ì—ê²Œ ë²•ë¥ ì„ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” êµìœ¡ìì…ë‹ˆë‹¤. ë³µì¡í•œ ë²•ì› íŒê²°ë¬¸ì„ ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆëŠ” ì¼ìƒ ì–¸ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1000
                )
            )

            content = response.choices[0].message.content
            return self._parse_summary_response(content)

        except Exception as e:
            self.logger.error(f"OpenAI summary API error: {e}")
            return {"summary": f"OpenAI ìš”ì•½ ì‹¤íŒ¨: {str(e)}"}

    async def _call_anthropic_for_summary(self, prompt: str) -> Dict[str, Any]:
        """Anthropic APIë¥¼ ì‚¬ìš©í•œ íŒë¡€ ìš”ì•½"""
        try:
            message = await asyncio.to_thread(
                lambda: self.anthropic_client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=1000,
                    temperature=0.3,
                    messages=[{"role": "user", "content": prompt}]
                )
            )

            content = message.content[0].text
            return self._parse_summary_response(content)

        except Exception as e:
            self.logger.error(f"Anthropic summary API error: {e}")
            return {"summary": f"Anthropic ìš”ì•½ ì‹¤íŒ¨: {str(e)}"}

    def _parse_summary_response(self, content: str) -> Dict[str, Any]:
        """AI ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ìš”ì•½ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ
            sections = {}
            current_section = None
            current_content = []

            for line in content.split('\n'):
                line = line.strip()
                if not line:
                    continue

                # ì„¹ì…˜ í—¤ë” ê°ì§€
                if 'ğŸ›ï¸ ì‚¬ê±´ ê°œìš”' in line:
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = 'case_overview'
                    current_content = []
                elif 'ğŸ¯ í•µì‹¬ ìŸì ' in line:
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = 'key_issues'
                    current_content = []
                elif 'âš–ï¸ íŒê²° ê²°ê³¼' in line:
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = 'outcome'
                    current_content = []
                elif 'ğŸ’¡ ì´ íŒë¡€ì˜ ì˜ë¯¸' in line:
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = 'significance'
                    current_content = []
                elif 'ğŸ“ í•œì¤„ ìš”ì•½' in line:
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = 'one_line_summary'
                    current_content = []
                else:
                    # ì¼ë°˜ ë‚´ìš© ë¼ì¸
                    if current_section and line:
                        current_content.append(line)

            # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
            if current_section and current_content:
                sections[current_section] = '\n'.join(current_content)

            # ê²°ê³¼ êµ¬ì„±
            case_overview = sections.get('case_overview', '').strip()
            key_issues = sections.get('key_issues', '').strip()
            outcome = sections.get('outcome', '').strip()
            significance = sections.get('significance', '').strip()
            one_line = sections.get('one_line_summary', '').strip()

            # ì „ì²´ ìš”ì•½ ìƒì„±
            full_summary = f"{case_overview}\n\n{key_issues}".strip()

            return {
                "summary": full_summary if full_summary else content[:500],
                "key_points": [key_issues] if key_issues else [],
                "outcome": outcome if outcome else "íŒê²° ê²°ê³¼ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "significance": significance if significance else "íŒë¡€ì˜ ì˜ë¯¸ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "one_line_summary": one_line if one_line else content[:100]
            }

        except Exception as e:
            self.logger.warning(f"Failed to parse AI response structure: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ë‚´ìš©ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
            return {
                "summary": content[:500] if content else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "key_points": [],
                "outcome": "íŒê²° ê²°ê³¼ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "significance": "íŒë¡€ì˜ ì˜ë¯¸ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "one_line_summary": content[:100] if content else "ìš”ì•½ ì—†ìŒ"
            }

    def _create_simple_summary(self, content: str, title: str = "") -> Dict[str, Any]:
        """AIê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ìš”ì•½"""
        try:
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„
            content_lower = content.lower()

            # íŒê²° ê²°ê³¼ í‚¤ì›Œë“œ í™•ì¸
            if any(keyword in content_lower for keyword in ['ìŠ¹ì†Œ', 'ì¸ìš©', 'ì·¨ì†Œ', 'ì§€ê¸‰']):
                outcome = "ì›ê³ (ì‹ ì²­ì¸)ì—ê²Œ ìœ ë¦¬í•œ íŒê²°"
            elif any(keyword in content_lower for keyword in ['íŒ¨ì†Œ', 'ê¸°ê°', 'ê°í•˜']):
                outcome = "ì›ê³ (ì‹ ì²­ì¸)ì—ê²Œ ë¶ˆë¦¬í•œ íŒê²°"
            else:
                outcome = "íŒê²° ê²°ê³¼ í™•ì¸ í•„ìš”"

            # ê°„ë‹¨ ìš”ì•½ ìƒì„±
            summary = f"ì œëª©: {title}\n\n" if title else ""
            summary += f"ë‚´ìš©: {content[:200]}..." if len(content) > 200 else content

            return {
                "summary": summary,
                "key_points": ["AI ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê°„ë‹¨ ìš”ì•½ë§Œ ì œê³µ"],
                "outcome": outcome,
                "significance": "ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ê°€ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "one_line_summary": f"{title} - {outcome}" if title else outcome
            }

        except Exception as e:
            return {
                "summary": f"ê°„ë‹¨ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "key_points": [],
                "outcome": "íŒê²° ê²°ê³¼ í™•ì¸ ë¶ˆê°€",
                "significance": "ë¶„ì„ ë¶ˆê°€",
                "one_line_summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
            }


# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
analysis_service = AnalysisService()