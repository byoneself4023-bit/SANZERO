#!/usr/bin/env python3
"""
PrecedentSearchService - í•˜ì´ë¸Œë¦¬ë“œ íŒë¡€ ê²€ìƒ‰ ì„œë¹„ìŠ¤
ê¸°ì¡´ RAG ê¸°ë°˜ ë¶„ì„ + ìƒˆë¡œìš´ TF-IDF ê²€ìƒ‰ì„ í†µí•©í•œ ì¢…í•© ì„œë¹„ìŠ¤
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path

# SANZERO ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤
from app.services.analysis_service import AnalysisService
from app.services.searcher import WorkInjuryCaseSearcher, PrecedentResult, get_searcher
from app.services.advanced_case_searcher import (
    AdvancedCaseSearcher, AdvancedPrecedentResult, DynamicThresholdResult,
    get_advanced_searcher
)

# ë¡œê¹… ì„¤ì • (ë¨¼ì € ì •ì˜)
logger = logging.getLogger(__name__)

# Enhanced searcher ì œê±°ë¨ - Basic searcherë§Œ ì‚¬ìš©
ENHANCED_SEARCHER_AVAILABLE = False

@dataclass
class HybridSearchResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë°ì´í„° í´ë˜ìŠ¤"""
    # ë©”íƒ€ë°ì´í„°
    query: str
    search_timestamp: str
    total_processing_time: float

    # TF-IDF ê²€ìƒ‰ ê²°ê³¼ (ë¹ ë¥¸ ê²€ìƒ‰) - ê³ ê¸‰ ê²°ê³¼ë¡œ ë³€ê²½
    tfidf_results: List[AdvancedPrecedentResult]
    tfidf_search_time: float
    tfidf_available: bool

    # ë™ì  ì„ê³„ê°’ ì •ë³´ (ìƒˆë¡œ ì¶”ê°€)
    dynamic_threshold: Optional[DynamicThresholdResult]

    # RAG ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ (ì‹¬í™” ë¶„ì„)
    rag_results: Optional[Dict[str, Any]]
    rag_analysis_time: float
    rag_available: bool

    # í†µí•© ë¶„ì„
    combined_insights: Dict[str, Any]
    recommendation: str
    confidence_score: float


class PrecedentSearchService:
    """
    í•˜ì´ë¸Œë¦¬ë“œ íŒë¡€ ê²€ìƒ‰ ì„œë¹„ìŠ¤

    ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì„ í†µí•©:
    1. TF-IDF ê¸°ë°˜ ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ (27,339ê±´ ì‹¤ì œ íŒë¡€)
    2. RAG ê¸°ë°˜ LLM ë¶„ì„ (ê¸°ì¡´ SANZERO ë¶„ì„)

    ì‚¬ìš©ìì—ê²Œ ë¹ ë¥¸ ê²€ìƒ‰ê³¼ ì‹¬í™” ë¶„ì„ì„ ëª¨ë‘ ì œê³µ
    """

    def __init__(self):
        """PrecedentSearchService ì´ˆê¸°í™”"""
        self.analysis_service = AnalysisService()
        self.tfidf_searcher = None
        self.advanced_searcher = None
        self.is_tfidf_available = False
        self.is_advanced_available = False

        # ì´ˆê¸°í™”
        self._initialize_services()

    def _initialize_services(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ê³ ê¸‰ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ìš°ì„ )
            self.advanced_searcher = get_advanced_searcher()
            self.is_advanced_available = self.advanced_searcher.is_loaded

            if self.is_advanced_available:
                logger.info("âœ… Advanced searcher with dynamic thresholds initialized successfully")
                # Advanced searcher í†µê³„ ë¡œê·¸ ì¶”ê°€
                stats = self.advanced_searcher.get_statistics()
                if stats.get("status") == "loaded":
                    logger.info(f"   ğŸ“Š Advanced searcher loaded: {stats.get('total_precedents', 0)} precedents, "
                              f"vocab: {stats.get('vocabulary_size', 0)}")
            else:
                logger.warning("âš ï¸ Advanced searcher loaded but not ready, falling back to basic searcher")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize Advanced searcher: {e}")
            logger.error(f"   Error type: {type(e).__name__}")
            logger.error(f"   Falling back to Basic searcher for compatibility")
            self.is_advanced_available = False

        # ê¸°ë³¸ TF-IDF ê²€ìƒ‰ê¸° (fallback)
        if not self.is_advanced_available:
            try:
                self.tfidf_searcher = get_searcher()
                self.is_tfidf_available = self.tfidf_searcher.is_loaded

                if self.is_tfidf_available:
                    logger.info("âœ… Basic TF-IDF searcher initialized successfully as fallback")
                    # Basic searcher í†µê³„ ë¡œê·¸ ì¶”ê°€
                    basic_stats = self.tfidf_searcher.get_statistics()
                    if basic_stats.get("status") == "loaded":
                        logger.info(f"   ğŸ“Š Basic searcher loaded: {basic_stats.get('total_cases', 0)} cases, "
                                  f"vocab: {basic_stats.get('vocabulary_size', 0)}")
                else:
                    logger.error("âŒ Basic TF-IDF searcher failed to load - no searcher available!")

            except Exception as e:
                logger.error(f"âŒ Failed to initialize basic TF-IDF searcher: {e}")
                logger.error(f"   Error type: {type(e).__name__}")
                logger.error(f"   No search functionality available!")
                self.is_tfidf_available = False

        # Enhanced ì‹œìŠ¤í…œ ì œê±°ë¨ - Basic TF-IDF searcherë§Œ ì‚¬ìš©
        logger.info("Using optimized TF-IDF searcher for fast performance")

    async def hybrid_search(
        self,
        query: str,
        tfidf_top_k: int = 10,
        include_rag_analysis: bool = True,
        timeout_seconds: int = 30,
        accuracy_level: str = "medium"
    ) -> HybridSearchResult:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë™ì  ì„ê³„ê°’ ì ìš©)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            tfidf_top_k: TF-IDF ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            include_rag_analysis: RAG ë¶„ì„ í¬í•¨ ì—¬ë¶€
            timeout_seconds: ì „ì²´ íƒ€ì„ì•„ì›ƒ
            accuracy_level: ì •í™•ë„ ìˆ˜ì¤€ (high/medium/low)

        Returns:
            HybridSearchResult: ë™ì  ì„ê³„ê°’ì´ ì ìš©ëœ í†µí•© ê²€ìƒ‰ ê²°ê³¼
        """
        start_time = datetime.now()

        logger.info(f"Starting hybrid search with dynamic threshold for query: '{query}'")

        # ê²°ê³¼ ì´ˆê¸°í™”
        tfidf_results = []
        tfidf_time = 0.0
        rag_results = None
        rag_time = 0.0
        dynamic_threshold_result = None

        # 1. ê³ ê¸‰ TF-IDF ê²€ìƒ‰ (ë™ì  ì„ê³„ê°’ ì ìš©)
        if self.is_advanced_available:
            tfidf_start = datetime.now()
            try:
                tfidf_results, dynamic_threshold_result = await self._perform_advanced_search(
                    query, tfidf_top_k, accuracy_level
                )
                tfidf_time = (datetime.now() - tfidf_start).total_seconds()
                logger.info(f"Advanced search completed in {tfidf_time:.2f}s, "
                          f"found {len(tfidf_results)} results with threshold {dynamic_threshold_result.final_threshold:.3f}")
            except Exception as e:
                logger.error(f"Advanced search failed: {e}")
                tfidf_results = []

        # Fallback: ê¸°ë³¸ TF-IDF ê²€ìƒ‰
        elif self.is_tfidf_available:
            tfidf_start = datetime.now()
            try:
                basic_results = await self._perform_tfidf_search(query, tfidf_top_k)
                # ê¸°ë³¸ ê²°ê³¼ë¥¼ ê³ ê¸‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                tfidf_results = self._convert_to_advanced_results(basic_results)
                tfidf_time = (datetime.now() - tfidf_start).total_seconds()
                logger.info(f"Basic TF-IDF search completed in {tfidf_time:.2f}s, found {len(tfidf_results)} results")
            except Exception as e:
                logger.error(f"Basic TF-IDF search failed: {e}")
                tfidf_results = []

        # 2. RAG ë¶„ì„ (ì„ íƒì )
        if include_rag_analysis:
            rag_start = datetime.now()
            try:
                rag_results = await self._perform_rag_analysis(query, timeout_seconds)
                rag_time = (datetime.now() - rag_start).total_seconds()
                logger.info(f"RAG analysis completed in {rag_time:.2f}s")
            except Exception as e:
                logger.error(f"RAG analysis failed: {e}")
                rag_results = None

        # 3. ê²°ê³¼ í†µí•© ë° ë¶„ì„
        total_time = (datetime.now() - start_time).total_seconds()

        combined_insights = self._combine_insights(tfidf_results, rag_results, query)
        recommendation = self._generate_recommendation(tfidf_results, rag_results, combined_insights)
        confidence_score = self._calculate_confidence_score(tfidf_results, rag_results)

        # í†µí•© ê²°ê³¼ ìƒì„±
        result = HybridSearchResult(
            query=query,
            search_timestamp=start_time.isoformat(),
            total_processing_time=total_time,

            tfidf_results=tfidf_results,
            tfidf_search_time=tfidf_time,
            tfidf_available=self.is_advanced_available or self.is_tfidf_available,

            dynamic_threshold=dynamic_threshold_result,

            rag_results=rag_results,
            rag_analysis_time=rag_time,
            rag_available=include_rag_analysis and rag_results is not None,

            combined_insights=combined_insights,
            recommendation=recommendation,
            confidence_score=confidence_score
        )

        logger.info(f"Hybrid search completed in {total_time:.2f}s (confidence: {confidence_score:.2f})")
        return result

    async def _perform_tfidf_search(self, query: str, top_k: int) -> List[PrecedentResult]:
        """TF-IDF ê²€ìƒ‰ ìˆ˜í–‰ (ê°•í™”ëœ ë²„ì „ ìš°ì„  ì‚¬ìš©)"""

        # TF-IDF ê²€ìƒ‰ (ë¹ ë¥¸ ì„±ëŠ¥ì„ ìœ„í•´ ìµœì í™”)
        if self.is_tfidf_available:
            logger.info("Using standard TF-IDF searcher")
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                self.tfidf_searcher.search,
                query,
                top_k
            )
            return results

        # 3. ê²€ìƒ‰ ë¶ˆê°€ëŠ¥
        logger.error("No searcher available (neither enhanced nor standard)")
        return []

    async def _perform_advanced_search(
        self,
        query: str,
        top_k: int,
        accuracy_level: str = "medium"
    ) -> Tuple[List[AdvancedPrecedentResult], DynamicThresholdResult]:
        """ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜í–‰ (ë™ì  ì„ê³„ê°’ ì ìš©)"""
        if not self.is_advanced_available:
            raise ValueError("Advanced searcher not available")

        user_preferences = {"accuracy_level": accuracy_level}

        loop = asyncio.get_event_loop()

        # ë¹„ë™ê¸°ë¡œ ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰
        search_results = await loop.run_in_executor(
            None,
            self.advanced_searcher.search,
            query,
            top_k,
            user_preferences
        )

        # ë™ì  ì„ê³„ê°’ ê²°ê³¼ë„ ë¹„ë™ê¸°ë¡œ ê³„ì‚°
        threshold_result = await loop.run_in_executor(
            None,
            self.advanced_searcher.calculate_dynamic_threshold,
            query,
            user_preferences
        )

        return search_results, threshold_result

    def _convert_to_advanced_results(self, basic_results: List[PrecedentResult]) -> List[AdvancedPrecedentResult]:
        """ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê³ ê¸‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        advanced_results = []

        logger.info(f"Converting {len(basic_results)} basic results to advanced format")

        for i, basic in enumerate(basic_results):
            try:
                # ì•ˆì „í•œ í•„ë“œ ì ‘ê·¼
                case_id = getattr(basic, 'case_id', f'CASE_{i}')
                title = getattr(basic, 'title', 'Unknown Title')
                content = getattr(basic, 'content', '')
                court = getattr(basic, 'court', 'Unknown Court')
                date = getattr(basic, 'date', 'Unknown Date')
                similarity = getattr(basic, 'similarity', 0.0)
                category = getattr(basic, 'category', 'Unknown')
                keywords = getattr(basic, 'keywords', [])

                # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
                title = str(title) if title else 'Unknown Title'
                content = str(content) if content else ''
                court = str(court) if court else 'Unknown Court'
                date = str(date) if date else 'Unknown Date'

                # ê¸°ë³¸ ìœ ë¶ˆë¦¬ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)
                favorability = "ì• ë§¤ â–³"  # ê¸°ë³¸ê°’
                favorable_score = 0
                unfavorable_score = 0

                if content:
                    content_lower = content.lower()
                    # ê·¼ë¡œìì—ê²Œ ìœ ë¦¬í•œ í‚¤ì›Œë“œ
                    favorable_keywords = ["ìŠ¹ì†Œ", "ì¸ìš©", "ë°°ìƒ", "ì¸ì •", "ì§€ê¸‰", "ì±…ì„", "ê³¼ì‹¤", "ìœ„ë°˜"]
                    # ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•œ í‚¤ì›Œë“œ
                    unfavorable_keywords = ["íŒ¨ì†Œ", "ê¸°ê°", "ë©´ì±…", "ë¬´ê³¼ì‹¤", "ê¸°ì—¬ê³¼ì‹¤", "ìê¸°ê³¼ì‹¤", "ê±°ë¶€"]

                    for keyword in favorable_keywords:
                        favorable_score += content_lower.count(keyword)
                    for keyword in unfavorable_keywords:
                        unfavorable_score += content_lower.count(keyword)

                    # íŒë‹¨ ë¡œì§ ê°œì„ 
                    if favorable_score > unfavorable_score + 1:
                        favorability = "ìœ ë¦¬ O"
                    elif unfavorable_score > favorable_score + 1:
                        favorability = "ë¶ˆë¦¬ X"
                    else:
                        favorability = "ì• ë§¤ â–³"

                # í‚¤ì›Œë“œ ì²˜ë¦¬ ê°œì„ 
                if isinstance(keywords, list) and keywords:
                    top_keywords = keywords[:5]
                    match_keywords = ', '.join(str(kw) for kw in keywords[:3] if kw)
                else:
                    top_keywords = []
                    match_keywords = '-'

                if not match_keywords or match_keywords.strip() == ',':
                    match_keywords = '-'

                advanced = AdvancedPrecedentResult(
                    case_id=case_id,
                    title=title,
                    content=content[:1000] + ("..." if len(content) > 1000 else ""),
                    court=court,
                    date=date,
                    similarity=float(similarity),
                    similarity_pct=round(float(similarity) * 100, 2),
                    category=category,
                    keywords=top_keywords,
                    match_keywords=match_keywords,
                    worker_favorable=favorability,
                    favorability_score={"favorable": favorable_score, "unfavorable": unfavorable_score}
                )
                advanced_results.append(advanced)

                # ì²« ë²ˆì§¸ ê²°ê³¼ëŠ” ìƒì„¸ ë¡œê·¸ ì¶œë ¥
                if i == 0:
                    logger.info(f"   Sample conversion: Basic(title='{title[:30]}...', similarity={similarity:.3f}) "
                              f"â†’ Advanced(favorability='{favorability}', match_keywords='{match_keywords[:20]}...')")

            except Exception as e:
                logger.error(f"Failed to convert basic result {i}: {e}")
                logger.error(f"   Basic result fields: {vars(basic) if hasattr(basic, '__dict__') else str(basic)}")
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ê²°ê³¼ ìƒì„±
                advanced = AdvancedPrecedentResult(
                    case_id=f'ERROR_{i}',
                    title=f'Conversion Error {i}',
                    content=f'Failed to convert basic result: {str(e)}',
                    court='Unknown',
                    date='Unknown',
                    similarity=0.0,
                    similarity_pct=0.0,
                    category='Error',
                    keywords=[],
                    match_keywords='-',
                    worker_favorable="ì• ë§¤ â–³",
                    favorability_score={"favorable": 0, "unfavorable": 0}
                )
                advanced_results.append(advanced)

        logger.info(f"Successfully converted {len(advanced_results)} results to advanced format")
        return advanced_results

    async def _perform_rag_analysis(
        self,
        query: str,
        timeout_seconds: int
    ) -> Optional[Dict[str, Any]]:
        """RAG ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ê¸°ì¡´ SANZERO ë¶„ì„ ì„œë¹„ìŠ¤ ì‚¬ìš©
            result = await asyncio.wait_for(
                self.analysis_service.analyze_precedent(query),
                timeout=timeout_seconds
            )

            if result and result.get('status') == 'success':
                return result.get('data', {})
            else:
                logger.warning("RAG analysis returned no valid results")
                return None

        except asyncio.TimeoutError:
            logger.warning(f"RAG analysis timed out after {timeout_seconds}s")
            return None
        except Exception as e:
            logger.error(f"RAG analysis error: {e}")
            return None

    def _combine_insights(
        self,
        tfidf_results: List[AdvancedPrecedentResult],
        rag_results: Optional[Dict[str, Any]],
        query: str
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ TF-IDFì™€ RAG ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            "query_analysis": {
                "original_query": query,
                "query_length": len(query),
                "has_keywords": len(query.split()) > 1
            }
        }

        # ê³ ê¸‰ TF-IDF ê²°ê³¼ ë¶„ì„
        if tfidf_results:
            insights["tfidf_analysis"] = {
                "total_results": len(tfidf_results),
                "avg_similarity": round(sum(r.similarity for r in tfidf_results) / len(tfidf_results), 3),
                "max_similarity": round(max(r.similarity for r in tfidf_results), 3),
                "min_similarity": round(min(r.similarity for r in tfidf_results), 3),
                "high_similarity_count": sum(1 for r in tfidf_results if r.similarity > 0.5),
                "courts": list(set([r.court for r in tfidf_results if r.court != "Unknown Court"])),
                "top_similarities": [round(r.similarity, 3) for r in tfidf_results[:5]]
            }

            # ê·¼ë¡œì ìœ ë¶ˆë¦¬ ë¶„í¬ ë¶„ì„ (ì‹ ê·œ)
            from collections import Counter
            favorability_counts = Counter([r.worker_favorable for r in tfidf_results])
            insights["tfidf_analysis"]["favorability_distribution"] = dict(favorability_counts)

            # ì¹´í…Œê³ ë¦¬ ë¶„ì„
            categories = [r.category for r in tfidf_results if r.category]
            if categories:
                category_counts = Counter(categories)
                insights["tfidf_analysis"]["category_distribution"] = dict(category_counts.most_common(5))

            # ë§¤ì¹­ í‚¤ì›Œë“œ ë¶„ì„ (ì‹ ê·œ)
            all_match_keywords = []
            for r in tfidf_results:
                if r.match_keywords and r.match_keywords != '-':
                    all_match_keywords.extend([kw.strip() for kw in r.match_keywords.split(',')])

            if all_match_keywords:
                keyword_counts = Counter(all_match_keywords)
                insights["tfidf_analysis"]["top_matching_keywords"] = dict(keyword_counts.most_common(10))

        # RAG ê²°ê³¼ ë¶„ì„
        if rag_results:
            insights["rag_analysis"] = {
                "has_precedents": bool(rag_results.get("precedents")),
                "precedent_count": len(rag_results.get("precedents", [])),
                "has_analysis": bool(rag_results.get("analysis")),
                "has_recommendations": bool(rag_results.get("recommendations")),
                "analysis_summary": rag_results.get("analysis", "")[:200] + "..." if rag_results.get("analysis") else ""
            }

        # êµì°¨ ë¶„ì„
        insights["cross_analysis"] = {
            "both_available": bool(tfidf_results and rag_results),
            "consistency_check": self._check_consistency(tfidf_results, rag_results),
            "complementary_value": self._assess_complementary_value(tfidf_results, rag_results)
        }

        return insights

    def _check_consistency(
        self,
        tfidf_results: List[PrecedentResult],
        rag_results: Optional[Dict[str, Any]]
    ) -> str:
        """TF-IDFì™€ RAG ê²°ê³¼ì˜ ì¼ê´€ì„± ê²€í† """
        if not tfidf_results or not rag_results:
            return "insufficient_data"

        # ê°„ë‹¨í•œ ì¼ê´€ì„± ê²€ì‚¬
        tfidf_avg_sim = sum(r.similarity for r in tfidf_results) / len(tfidf_results)
        rag_has_good_precedents = len(rag_results.get("precedents", [])) > 0

        if tfidf_avg_sim > 0.3 and rag_has_good_precedents:
            return "consistent"
        elif tfidf_avg_sim < 0.1 and not rag_has_good_precedents:
            return "consistently_low"
        else:
            return "mixed"

    def _assess_complementary_value(
        self,
        tfidf_results: List[PrecedentResult],
        rag_results: Optional[Dict[str, Any]]
    ) -> str:
        """ë‘ ë°©ì‹ì˜ ìƒí˜¸ ë³´ì™„ì  ê°€ì¹˜ í‰ê°€"""
        if not tfidf_results and not rag_results:
            return "both_unavailable"
        elif tfidf_results and not rag_results:
            return "tfidf_only"
        elif not tfidf_results and rag_results:
            return "rag_only"
        else:
            return "complementary"

    def _generate_recommendation(
        self,
        tfidf_results: List[AdvancedPrecedentResult],
        rag_results: Optional[Dict[str, Any]],
        insights: Dict[str, Any]
    ) -> str:
        """ê³ ê¸‰ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ê³ ê¸‰ TF-IDF ê¸°ë°˜ ê¶Œê³ ì‚¬í•­
        if tfidf_results:
            avg_sim = insights.get("tfidf_analysis", {}).get("avg_similarity", 0)
            if avg_sim > 0.5:
                recommendations.append("ë†’ì€ ìœ ì‚¬ë„ì˜ íŒë¡€ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            elif avg_sim > 0.3:
                recommendations.append("ì ì •í•œ ìœ ì‚¬ë„ì˜ íŒë¡€ë“¤ì´ ì°¾ì•„ì¡ŒìŠµë‹ˆë‹¤.")
            else:
                recommendations.append("ìœ ì‚¬ë„ê°€ ë‚®ì€ í¸ì…ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")

            # ê·¼ë¡œì ìœ ë¶ˆë¦¬ ë¶„ì„ ê¸°ë°˜ ê¶Œê³ ì‚¬í•­ (ì‹ ê·œ)
            favorability_dist = insights.get("tfidf_analysis", {}).get("favorability_distribution", {})
            if favorability_dist:
                favorable_count = favorability_dist.get("ìœ ë¦¬ O", 0)
                unfavorable_count = favorability_dist.get("ë¶ˆë¦¬ X", 0)
                neutral_count = favorability_dist.get("ì• ë§¤ â–³", 0)

                total_cases = favorable_count + unfavorable_count + neutral_count
                if total_cases > 0:
                    favorable_ratio = favorable_count / total_cases
                    if favorable_ratio > 0.6:
                        recommendations.append("ëŒ€ë¶€ë¶„ì˜ ìœ ì‚¬ íŒë¡€ì—ì„œ ê·¼ë¡œìì—ê²Œ ìœ ë¦¬í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.")
                    elif favorable_ratio < 0.3:
                        recommendations.append("ìœ ì‚¬ íŒë¡€ ì¤‘ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•œ ê²°ê³¼ê°€ ë§ìŠµë‹ˆë‹¤. ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    else:
                        recommendations.append("ìœ ì‚¬ íŒë¡€ì˜ ê²°ê³¼ê°€ í˜¼ì¬ë˜ì–´ ìˆì–´ ì‚¬ì•ˆë³„ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # RAG ê¸°ë°˜ ê¶Œê³ ì‚¬í•­
        if rag_results and rag_results.get("recommendations"):
            rag_rec = rag_results["recommendations"]
            if isinstance(rag_rec, list) and rag_rec:
                recommendations.append(f"AI ë¶„ì„: {rag_rec[0][:100]}...")
            elif isinstance(rag_rec, str):
                recommendations.append(f"AI ë¶„ì„: {rag_rec[:100]}...")

        # í†µí•© ê¶Œê³ ì‚¬í•­
        consistency = insights.get("cross_analysis", {}).get("consistency_check", "")
        if consistency == "consistent":
            recommendations.append("TF-IDF ê²€ìƒ‰ê³¼ AI ë¶„ì„ì´ ì¼ì¹˜í•˜ì—¬ ì‹ ë¢°ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
        elif consistency == "mixed":
            recommendations.append("ë‹¤ì–‘í•œ ê´€ì ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")

        # ê¸°ë³¸ ê¶Œê³ ì‚¬í•­
        if not recommendations:
            recommendations.append("ì¶”ê°€ì ì¸ ê²€ìƒ‰ì–´ë‚˜ ë” êµ¬ì²´ì ì¸ ì‚¬ê³  ìƒí™©ì„ ì…ë ¥í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.")

        return " ".join(recommendations)

    def _calculate_confidence_score(
        self,
        tfidf_results: List[AdvancedPrecedentResult],
        rag_results: Optional[Dict[str, Any]]
    ) -> float:
        """ê³ ê¸‰ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        score = 0.0

        # ê³ ê¸‰ TF-IDF ì‹ ë¢°ë„ (ìµœëŒ€ 0.6ì )
        if tfidf_results:
            avg_similarity = sum(r.similarity for r in tfidf_results) / len(tfidf_results)
            result_count_bonus = min(len(tfidf_results) / 10, 0.15)  # ê²°ê³¼ ìˆ˜ ë³´ë„ˆìŠ¤

            # ë§¤ì¹­ í‚¤ì›Œë“œ í’ˆì§ˆ ë³´ë„ˆìŠ¤ (ì‹ ê·œ)
            keyword_quality_bonus = 0.0
            for result in tfidf_results:
                if result.match_keywords and result.match_keywords != '-':
                    keyword_count = len(result.match_keywords.split(','))
                    keyword_quality_bonus += min(keyword_count * 0.02, 0.05)
            keyword_quality_bonus = min(keyword_quality_bonus / len(tfidf_results), 0.1)

            # ê·¼ë¡œì ìœ ë¶ˆë¦¬ ì¼ê´€ì„± ë³´ë„ˆìŠ¤ (ì‹ ê·œ)
            favorable_results = [r for r in tfidf_results if r.worker_favorable in ["ìœ ë¦¬ O", "ë¶ˆë¦¬ X"]]
            consistency_bonus = 0.0
            if len(favorable_results) >= 3:
                consistency_bonus = 0.05

            tfidf_score = avg_similarity + result_count_bonus + keyword_quality_bonus + consistency_bonus
            score += min(tfidf_score, 0.6)

        # RAG ì‹ ë¢°ë„ (ìµœëŒ€ 0.4ì )
        if rag_results:
            rag_score = 0.0
            if rag_results.get("precedents"):
                rag_score += 0.15  # íŒë¡€ ì¡´ì¬
            if rag_results.get("analysis"):
                rag_score += 0.15  # ë¶„ì„ ì¡´ì¬
            if rag_results.get("recommendations"):
                rag_score += 0.1  # ê¶Œê³ ì‚¬í•­ ì¡´ì¬
            score += rag_score

        return min(score, 1.0)

    async def quick_search(self, query: str, top_k: int = 5) -> List[PrecedentResult]:
        """ë¹ ë¥¸ TF-IDF ê²€ìƒ‰ë§Œ ìˆ˜í–‰"""
        if not self.is_tfidf_available:
            return []

        return await self._perform_tfidf_search(query, top_k)

    async def deep_analysis(
        self,
        query: str,
        timeout_seconds: int = 30
    ) -> Optional[Dict[str, Any]]:
        """ì‹¬í™” RAG ë¶„ì„ë§Œ ìˆ˜í–‰"""
        return await self._perform_rag_analysis(query, timeout_seconds)

    def get_search_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì„œë¹„ìŠ¤ í†µê³„ ë°˜í™˜"""
        stats = {
            "service_name": "PrecedentSearchService",
            "version": "3.0.0",  # Enhanced ì œê±°, ìµœì í™”ëœ ë²„ì „
            "capabilities": {
                "tfidf_search": self.is_tfidf_available,
                "rag_analysis": True,
                "hybrid_search": self.is_tfidf_available,
                "optimized_performance": True  # ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
            }
        }

        # TF-IDF í†µê³„ (ìµœì í™”ëœ ì„±ëŠ¥)
        if self.is_tfidf_available:
            try:
                tfidf_stats = self.tfidf_searcher.get_statistics()
                stats["tfidf_statistics"] = tfidf_stats
                stats["primary_searcher"] = "optimized_tfidf"
            except Exception as e:
                logger.error(f"Failed to get TF-IDF statistics: {e}")
                stats["tfidf_statistics"] = {"error": str(e)}

        return stats

    def to_dict(self, result: HybridSearchResult) -> Dict[str, Any]:
        """HybridSearchResultë¥¼ dictë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        data = asdict(result)

        # AdvancedPrecedentResult ê°ì²´ë“¤ì„ dictë¡œ ë³€í™˜
        data["tfidf_results"] = [
            {
                "case_id": r.case_id,
                "title": r.title,
                "content": r.content[:300] + "..." if len(r.content) > 300 else r.content,
                "court": r.court,
                "date": r.date,
                "similarity": r.similarity,
                "similarity_pct": r.similarity_pct,
                "category": r.category,
                "keywords": r.keywords,
                "match_keywords": r.match_keywords,
                "worker_favorable": r.worker_favorable,
                "favorability_score": r.favorability_score
            }
            for r in result.tfidf_results
        ]

        # DynamicThresholdResult ì§ë ¬í™”
        if result.dynamic_threshold:
            data["dynamic_threshold"] = asdict(result.dynamic_threshold)

        return data


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_precedent_service_instance: Optional[PrecedentSearchService] = None

def get_precedent_service() -> PrecedentSearchService:
    """
    PrecedentSearchService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    FastAPI ì˜ì¡´ì„± ì£¼ì…ìš©
    """
    global _precedent_service_instance

    if _precedent_service_instance is None:
        _precedent_service_instance = PrecedentSearchService()

    return _precedent_service_instance


# í¸ì˜ í•¨ìˆ˜ë“¤
async def hybrid_precedent_search(
    query: str,
    tfidf_top_k: int = 10,
    include_rag_analysis: bool = True,
    timeout_seconds: int = 30
) -> HybridSearchResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    service = get_precedent_service()
    return await service.hybrid_search(
        query=query,
        tfidf_top_k=tfidf_top_k,
        include_rag_analysis=include_rag_analysis,
        timeout_seconds=timeout_seconds
    )

async def quick_precedent_search(query: str, top_k: int = 5) -> List[PrecedentResult]:
    """ë¹ ë¥¸ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    service = get_precedent_service()
    return await service.quick_search(query, top_k)

async def deep_precedent_analysis(
    query: str,
    timeout_seconds: int = 30
) -> Optional[Dict[str, Any]]:
    """ì‹¬í™” ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    service = get_precedent_service()
    return await service.deep_analysis(query, timeout_seconds=timeout_seconds)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_service():
        service = PrecedentSearchService()

        # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        stats = service.get_search_statistics()
        print("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„:")
        for key, value in stats.items():
            print(f"  {key}: {value}")

        # ë¹ ë¥¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ” ë¹ ë¥¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
        quick_results = await service.quick_search("ì‘ì—… ì¤‘ ì†ê°€ë½ ë‹¤ì¹¨", top_k=3)
        print(f"ê²°ê³¼ ìˆ˜: {len(quick_results)}")
        for i, result in enumerate(quick_results, 1):
            print(f"{i}. ìœ ì‚¬ë„: {result.similarity:.3f} | {result.title[:50]}...")

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
        hybrid_result = await service.hybrid_search(
            "ì¶”ë½ ì‚¬ê³ ë¡œ ì¸í•œ ë¨¸ë¦¬ ë¶€ìƒ",
            tfidf_top_k=5,
            include_rag_analysis=False  # RAG ë¶„ì„ ì œì™¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        )

        print(f"ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {hybrid_result.total_processing_time:.2f}s")
        print(f"TF-IDF ê²°ê³¼: {len(hybrid_result.tfidf_results)}ê°œ")
        print(f"ì‹ ë¢°ë„ ì ìˆ˜: {hybrid_result.confidence_score:.2f}")
        print(f"ê¶Œê³ ì‚¬í•­: {hybrid_result.recommendation}")

    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    asyncio.run(test_service())