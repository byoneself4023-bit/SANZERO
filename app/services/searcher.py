#!/usr/bin/env python3
"""
WorkInjuryCaseSearcher ì„œë¹„ìŠ¤
Test_casePedia.ipynbì—ì„œ ì´ì‹ëœ ì‚°ì¬ íŒë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ
SANZERO FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ í†µí•©í•˜ê¸° ìœ„í•´ ìµœì í™”ë¨
"""

import pickle
import joblib
import zlib
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import re
import logging
from dataclasses import dataclass
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity

# í•œê¸€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜ì¡´ì„± - ì„ íƒì  import
try:
    from konlpy.tag import Okt
    from konlpy.tag import Mecab
    KOREAN_NLP_AVAILABLE = True
except ImportError:
    KOREAN_NLP_AVAILABLE = False
    logging.warning("Korean NLP libraries (konlpy) not available. Using basic tokenization.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PrecedentResult:
    """íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    case_id: str
    title: str
    content: str
    court: str
    date: str
    similarity: float
    category: Optional[str] = None
    keywords: Optional[List[str]] = None


class WorkInjuryCaseSearcher:
    """
    ì‚°ì¬ íŒë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ

    27,339ê°œì˜ ì‹¤ì œ íŒë¡€ ë°ì´í„°ë¥¼ TF-IDF ë²¡í„°í™”í•˜ì—¬
    ë¹ ë¥´ê³  ì •í™•í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.

    ì£¼ìš” ê¸°ëŠ¥:
    - í•œê¸€ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (Okt/Mecab ì§€ì›)
    - TF-IDF ê¸°ë°˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
    - ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
    - ê²€ìƒ‰ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
    """

    def __init__(self, model_path: Optional[str] = None):
        """
        WorkInjuryCaseSearcher ì´ˆê¸°í™”

        Args:
            model_path: searcher_model.pkl íŒŒì¼ ê²½ë¡œ
        """
        self.model_path = model_path or self._get_default_model_path()
        self.df = None
        self.vectorizer = None
        self.tfidf_matrix = None
        self.config = None
        self.tokenizer = None
        self.is_loaded = False

        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê´€ë ¨ ì†ì„±ë“¤
        self.domain_keywords = []
        self.boost_multiplier = 2.0
        self.stopwords = set()

        # í•œê¸€ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self._init_tokenizer()

    def _get_default_model_path(self) -> str:
        """ê¸°ë³¸ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        current_dir = Path(__file__).parent
        app_dir = current_dir.parent
        return str(app_dir / "searcher_model.pkl")

    def _init_tokenizer(self):
        """í•œê¸€ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
        if not KOREAN_NLP_AVAILABLE:
            logger.warning("Korean NLP not available. Using basic tokenization.")
            return

        try:
            # Mecabì´ ìˆìœ¼ë©´ ì‚¬ìš© (ë” ì •í™•í•¨)
            self.tokenizer = Mecab()
            logger.info("Mecab tokenizer initialized successfully")
        except Exception as e:
            try:
                # Mecabì´ ì—†ìœ¼ë©´ Okt ì‚¬ìš©
                self.tokenizer = Okt()
                logger.info("Okt tokenizer initialized successfully")
            except Exception as e2:
                logger.warning(f"Failed to initialize Korean tokenizers: {e}, {e2}")
                self.tokenizer = None

    def load_model(self) -> bool:
        """
        searcher_model.pkl íŒŒì¼ì„ ë¡œë“œ

        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"Loading model from: {self.model_path}")

            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(self.model_path).exists():
                logger.error(f"Model file not found: {self.model_path}")
                return False

            file_size = Path(self.model_path).stat().st_size
            logger.info(f"File size: {file_size / 1024 / 1024:.2f} MB")

            # joblibìœ¼ë¡œ ë¡œë“œ (ì›ë³¸ ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ ë°©ì‹)
            logger.info("Loading with joblib (original method)...")
            model_data = joblib.load(self.model_path)

            # ë°ì´í„° í• ë‹¹
            self.df = model_data.get('df')
            self.vectorizer = model_data.get('vectorizer')
            self.tfidf_matrix = model_data.get('tfidf_matrix')
            self.config = model_data.get('config', {})

            # ë¡œë“œ ê²€ì¦
            if self.df is None or self.vectorizer is None or self.tfidf_matrix is None:
                logger.error("Required model components missing")
                return False

            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì„¤ì • ë¡œë“œ
            self._load_keyword_boosting_config()

            self.is_loaded = True
            logger.info(f"Model loaded successfully: {len(self.df)} cases, "
                       f"vocabulary: {len(self.vectorizer.vocabulary_):,}")

            return True

        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            return False

    def _load_keyword_boosting_config(self):
        """í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì„¤ì • ë¡œë“œ"""
        try:
            # configì—ì„œ í‚¤ì›Œë“œ ì •ë³´ ë¡œë“œ
            self.domain_keywords = self.config.get('domain_keywords', [])
            self.boost_multiplier = self.config.get('boost_multiplier', 2.0)
            self.stopwords = set(self.config.get('stopwords', []))

            # configì— í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ì„¤ì •
            if not self.domain_keywords:
                self.domain_keywords = [
                    "ì‚°ì—…ì¬í•´", "ì‘ì—…", "ë¶€ìƒ", "ì‚¬ê³ ", "ì¬í•´", "ì•ˆì „", "ê·¼ë¡œ", "ì—…ë¬´",
                    "ê¸°ê³„", "ì¶”ë½", "ë¼ì„", "ì ˆë‹¨", "í™”ìƒ", "ê°ì „", "ì§ˆì‹", "ì¤‘ë…",
                    "í”„ë ˆìŠ¤", "í¬ë ˆì¸", "ì‚¬ë‹¤ë¦¬", "ë¹„ê³„", "ìš©ì ‘", "ì ˆì‚­", "ì—°ì‚­",
                    "ì†ê°€ë½", "íŒ”", "ë‹¤ë¦¬", "ë¨¸ë¦¬", "í—ˆë¦¬", "ëª©", "ì–´ê¹¨", "ë¬´ë¦",
                    "ê³¨ì ˆ", "íƒ€ë°•", "ì—´ìƒ", "í™”ìƒ", "ì—¼ì¢Œ", "íƒˆêµ¬", "íŒŒì—´", "ì ˆë‹¨",
                    "ì‚°ì¬ë³´í—˜", "ë³´ìƒê¸ˆ", "ì¹˜ë£Œë¹„", "ìš”ì–‘ê¸‰ì—¬", "ì¥í•´ê¸‰ì—¬", "íœ´ì—…ê¸‰ì—¬",
                    "ê·¼ë¡œë³µì§€ê³µë‹¨", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•", "ìŠ¹ì¸", "ë¶ˆìŠ¹ì¸", "ì¬ì‹¬", "ì´ì˜ì‹ ì²­"
                ]

            # ê¸°ë³¸ ë¶ˆìš©ì–´ ì„¤ì •
            if not self.stopwords:
                self.stopwords = {
                    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ëŠ”', 'ì€', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤',
                    'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ì„œ',
                    'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë”°ë¼',
                    'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ëŸ°ë°'
                }

            logger.info(f"Keyword boosting configured: {len(self.domain_keywords)} domain keywords, "
                       f"boost multiplier: {self.boost_multiplier}")

        except Exception as e:
            logger.warning(f"Failed to load keyword boosting config: {e}, using defaults")
            # ê¸°ë³¸ê°’ ì„¤ì •
            self.domain_keywords = ["ì‚°ì—…ì¬í•´", "ì‘ì—…", "ë¶€ìƒ", "ì‚¬ê³ ", "ì¬í•´"]
            self.boost_multiplier = 2.0
            self.stopwords = {'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ëŠ”', 'ì€'}

    def _tokenize(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•

        Args:
            text: í† í¬ë‚˜ì´ì§•í•  í…ìŠ¤íŠ¸

        Returns:
            List[str]: í† í° ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        # ê¸°ë³¸ ì „ì²˜ë¦¬
        text = re.sub(r'[^\w\s]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'\s+', ' ', text)      # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = text.strip().lower()

        if self.tokenizer and KOREAN_NLP_AVAILABLE:
            try:
                # í•œê¸€ í˜•íƒœì†Œ ë¶„ì„
                if hasattr(self.tokenizer, 'morphs'):
                    tokens = self.tokenizer.morphs(text)
                else:
                    tokens = self.tokenizer.nouns(text)

                # ê¸¸ì´ 2 ì´ìƒì˜ í† í°ë§Œ ìœ ì§€í•˜ê³  ë¶ˆìš©ì–´ ì œê±°
                tokens = [token for token in tokens
                         if len(token) >= 2 and token not in self.stopwords]

                # ë„ë©”ì¸ í‚¤ì›Œë“œ ì¤‘ë³µ ì¶”ê°€ (Test_casePedia ë°©ì‹)
                domain_words = [token for token in tokens if token in self.domain_keywords]
                final_tokens = list(set(tokens + domain_words))  # ì¤‘ë³µ ì œê±°

                return final_tokens

            except Exception as e:
                logger.warning(f"Korean tokenization failed: {e}, using basic split")

        # ê¸°ë³¸ í† í¬ë‚˜ì´ì§• (fallback)
        tokens = text.split()
        tokens = [token for token in tokens
                 if len(token) >= 2 and token not in self.stopwords]

        # ê¸°ë³¸ í† í°í™”ì—ë„ ë„ë©”ì¸ í‚¤ì›Œë“œ ì¤‘ë³µ ì¶”ê°€ ì ìš©
        domain_words = [token for token in tokens if token in self.domain_keywords]
        final_tokens = list(set(tokens + domain_words))  # ì¤‘ë³µ ì œê±°

        return final_tokens

    def search(self, query: str, top_k: int = 10, category_filter: Optional[str] = None) -> List[PrecedentResult]:
        """
        íŒë¡€ ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            category_filter: ì¹´í…Œê³ ë¦¬ í•„í„°

        Returns:
            List[PrecedentResult]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_loaded:
            logger.error("Model not loaded. Call load_model() first.")
            return []

        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            query_tokens = self._tokenize(query)
            if not query_tokens:
                logger.warning("No valid tokens found in query")
                return []

            processed_query = ' '.join(query_tokens)
            query_set = set(query_tokens)
            logger.info(f"Processed query: '{processed_query}' (from: '{query}')")

            # TF-IDF ë²¡í„°í™”
            query_vector = self.vectorizer.transform([processed_query])

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì˜¬ë°”ë¥¸ ë°©ë²•)
            base_similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]

            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš©
            similarities = self._apply_keyword_boosting(base_similarities, query_set, query_tokens)

            # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            df_filtered = self.df.copy()
            if category_filter and 'category' in df_filtered.columns:
                df_filtered = df_filtered[df_filtered['category'] == category_filter]
                # í•„í„°ë§ëœ ì¸ë±ìŠ¤ì— ë§ì¶° ìœ ì‚¬ë„ ì¡°ì •
                filtered_indices = df_filtered.index.tolist()
                similarities = similarities[filtered_indices]

            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            top_indices = np.argsort(similarities)[::-1][:top_k]

            # ê²°ê³¼ ìƒì„±
            results = []
            for idx in top_indices:
                if float(similarities[idx]) >= 0.001:  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì¶”ê°€ ì™„í™” (Test_casePedia ìˆ˜ì¤€ ë§¤ì¹­)
                    original_idx = df_filtered.iloc[idx].name if category_filter else idx
                    case_data = self.df.iloc[original_idx]

                    result = PrecedentResult(
                        case_id=f"CASE_{original_idx}",
                        title=str(case_data.get('title', 'Unknown')),
                        content=str(case_data.get('noncontent', '')),
                        court=str(case_data.get('courtname', 'Unknown Court')),
                        date=str(case_data.get('kinda', 'Unknown Date')),
                        similarity=float(similarities[idx]),
                        category=case_data.get('category', 'Unknown'),
                        keywords=self._extract_keywords(processed_query)
                    )
                    results.append(result)

            logger.info(f"Found {len(results)} relevant cases for query: '{query}'")
            return results

        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text:
            return []

        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        tokens = self._tokenize(text)

        # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)
        stopwords = {'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ëŠ”', 'ì€', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤'}
        keywords = [token for token in tokens if token not in stopwords and len(token) >= 2]

        # ë¹ˆë„ ê³„ì‚° í›„ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        from collections import Counter
        keyword_counts = Counter(keywords)
        return [word for word, count in keyword_counts.most_common(10)]

    def _apply_keyword_boosting(
        self,
        base_similarities: np.ndarray,
        query_set: set,
        query_tokens: List[str]
    ) -> np.ndarray:
        """
        í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš© (Test_casePedia.ipynb ë°©ì‹)

        Args:
            base_similarities: ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²°ê³¼
            query_set: ì¿¼ë¦¬ í† í° ì§‘í•©
            query_tokens: ì¿¼ë¦¬ í† í° ë¦¬ìŠ¤íŠ¸

        Returns:
            np.ndarray: ë¶€ìŠ¤íŒ…ì´ ì ìš©ëœ ìœ ì‚¬ë„
        """
        boosted = base_similarities.copy()

        # tokens ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ì²˜ë¦¬
        if 'tokens' not in self.df.columns:
            logger.warning("No tokens column found, skipping keyword boosting")
            return boosted

        try:
            for i, similarity in enumerate(base_similarities):
                try:
                    # ë¬¸ì„œì˜ í† í° ê°€ì ¸ì˜¤ê¸° (Jupyter ë…¸íŠ¸ë¶ ë°©ì‹: ë‹¨ìˆœí•˜ê³  í™•ì‹¤í•¨)
                    tokens_data = self.df.iloc[i]['tokens']

                    # tokensê°€ Noneì´ê±°ë‚˜ ë¹ˆ ê°’ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                    if pd.isna(tokens_data) or not tokens_data:
                        continue

                    doc_tokens = set(tokens_data)

                    # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°
                    common_keywords = query_set & doc_tokens

                    if common_keywords:
                        # ë„ë©”ì¸ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°
                        domain_count = sum(1 for kw in common_keywords
                                         if kw in self.domain_keywords)

                        # ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
                        match_ratio = len(common_keywords) / max(len(query_tokens), 1)

                        # ë¶€ìŠ¤íŒ… ê³„ì‚° (Test_casePedia ë°©ì‹)
                        boost_factor = match_ratio * (1 + domain_count * 0.2) * (self.boost_multiplier - 1)

                        # ìœ ì‚¬ë„ ë¶€ìŠ¤íŒ… ì ìš© (ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ)
                        boosted[i] = min(similarity * (1 + boost_factor), 1.0)

                        # ì²« ë²ˆì§¸ ë¶€ìŠ¤íŒ… ì‚¬ë¡€ ë¡œê·¸
                        if i == 0 and boost_factor > 0:
                            logger.info(f"Keyword boosting applied: match_ratio={match_ratio:.3f}, "
                                      f"domain_count={domain_count}, boost_factor={boost_factor:.3f}, "
                                      f"original_sim={similarity:.3f} â†’ boosted_sim={boosted[i]:.3f}")

                except Exception as e:
                    # ê°œë³„ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì‚¬ë„ ìœ ì§€
                    logger.warning(f"Failed to apply boosting to document {i}: {e}")
                    continue

            # ë¶€ìŠ¤íŒ… í†µê³„ ë¡œê·¸
            boosted_count = sum(1 for i in range(len(base_similarities))
                              if abs(boosted[i] - base_similarities[i]) > 0.001)
            if boosted_count > 0:
                logger.info(f"Keyword boosting applied to {boosted_count}/{len(base_similarities)} documents")

        except Exception as e:
            logger.error(f"Keyword boosting failed: {e}, returning original similarities")
            return base_similarities

        return boosted

    def generate_report(self, query: str, top_n: int = 3) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_n: ë¶„ì„í•  ìƒìœ„ ê²°ê³¼ ìˆ˜

        Returns:
            Dict[str, Any]: ë¶„ì„ ë³´ê³ ì„œ
        """
        if not self.is_loaded:
            return {"error": "Model not loaded"}

        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.search(query, top_k=top_n * 2)  # ì—¬ìœ ìˆê²Œ ê²€ìƒ‰

            if not search_results:
                return {
                    "query": query,
                    "total_results": 0,
                    "message": "ê´€ë ¨ëœ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }

            # ìƒìœ„ ê²°ê³¼ ë¶„ì„
            top_results = search_results[:top_n]

            # ì¹´í…Œê³ ë¦¬ ë¶„ì„
            categories = {}
            for result in search_results:
                if result.category:
                    categories[result.category] = categories.get(result.category, 0) + 1

            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean([r.similarity for r in top_results])

            # ë²•ì›ë³„ ë¶„í¬
            courts = {}
            for result in top_results:
                courts[result.court] = courts.get(result.court, 0) + 1

            # í‚¤ì›Œë“œ í†µê³„
            all_keywords = []
            for result in top_results:
                if result.keywords:
                    all_keywords.extend(result.keywords)

            from collections import Counter
            keyword_stats = Counter(all_keywords).most_common(10)

            report = {
                "query": query,
                "search_time": datetime.now().isoformat(),
                "total_results": len(search_results),
                "analyzed_cases": len(top_results),
                "average_similarity": round(avg_similarity, 3),
                "top_cases": [
                    {
                        "case_id": r.case_id,
                        "title": r.title[:100] + "..." if len(r.title) > 100 else r.title,
                        "court": r.court,
                        "date": r.date,
                        "similarity": round(r.similarity, 3),
                        "summary": r.content[:200] + "..." if len(r.content) > 200 else r.content
                    }
                    for r in top_results
                ],
                "category_distribution": categories,
                "court_distribution": courts,
                "key_keywords": [{"keyword": k, "frequency": v} for k, v in keyword_stats],
                "recommendation": self._generate_recommendation(avg_similarity, categories)
            }

            return report

        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            return {"error": f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def _generate_recommendation(self, avg_similarity: float, categories: Dict[str, int]) -> str:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        if avg_similarity > 0.7:
            rec = "ë§¤ìš° ìœ ì‚¬í•œ íŒë¡€ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
        elif avg_similarity > 0.5:
            rec = "ì ì ˆí•œ ìœ ì‚¬ë„ì˜ íŒë¡€ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
        else:
            rec = "ìœ ì‚¬ë„ê°€ ë‚®ì€ íŒë¡€ë“¤ì…ë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”. "

        if categories:
            main_category = max(categories.keys(), key=lambda k: categories[k])
            rec += f"ì£¼ë¡œ '{main_category}' ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ ì‚¬ê±´ì…ë‹ˆë‹¤."

        return rec

    def get_statistics(self) -> Dict[str, Any]:
        """ëª¨ë¸ ë° ë°ì´í„° í†µê³„ ë°˜í™˜"""
        if not self.is_loaded:
            return {"error": "Model not loaded"}

        stats = {
            "total_cases": len(self.df),
            "vocabulary_size": len(self.vectorizer.vocabulary_) if self.vectorizer else 0,
            "tfidf_matrix_shape": self.tfidf_matrix.shape if self.tfidf_matrix is not None else None,
            "available_columns": list(self.df.columns) if self.df is not None else [],
            "korean_nlp_available": KOREAN_NLP_AVAILABLE,
            "tokenizer_type": type(self.tokenizer).__name__ if self.tokenizer else "basic",
            "config": self.config
        }

        # ì¹´í…Œê³ ë¦¬ í†µê³„
        if 'category' in self.df.columns:
            stats["categories"] = self.df['category'].value_counts().to_dict()

        # ë²•ì›ë³„ í†µê³„
        court_col = 'court' if 'court' in self.df.columns else 'ë²•ì›'
        if court_col in self.df.columns:
            stats["courts"] = self.df[court_col].value_counts().head(10).to_dict()

        return stats


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_searcher_instance: Optional[WorkInjuryCaseSearcher] = None


def get_searcher() -> WorkInjuryCaseSearcher:
    """
    WorkInjuryCaseSearcher ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    FastAPI ì˜ì¡´ì„± ì£¼ì…ìš©
    """
    global _searcher_instance

    if _searcher_instance is None:
        _searcher_instance = WorkInjuryCaseSearcher()

        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        if not _searcher_instance.load_model():
            logger.error("Failed to load searcher model on initialization")

    return _searcher_instance


def init_searcher(model_path: Optional[str] = None) -> bool:
    """
    Searcher ê°•ì œ ì´ˆê¸°í™”
    í…ŒìŠ¤íŠ¸ë‚˜ ìˆ˜ë™ ì„¤ì •ì‹œ ì‚¬ìš©
    """
    global _searcher_instance

    _searcher_instance = WorkInjuryCaseSearcher(model_path)
    return _searcher_instance.load_model()


# í¸ì˜ í•¨ìˆ˜ë“¤
def search_precedents(query: str, top_k: int = 10, category_filter: Optional[str] = None) -> List[PrecedentResult]:
    """íŒë¡€ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    searcher = get_searcher()
    return searcher.search(query, top_k, category_filter)


def generate_precedent_report(query: str, top_n: int = 3) -> Dict[str, Any]:
    """íŒë¡€ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    searcher = get_searcher()
    return searcher.generate_report(query, top_n)


def get_searcher_stats() -> Dict[str, Any]:
    """Searcher í†µê³„ ì¡°íšŒ í¸ì˜ í•¨ìˆ˜"""
    searcher = get_searcher()
    return searcher.get_statistics()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    searcher = WorkInjuryCaseSearcher()

    if searcher.load_model():
        print("ğŸ‰ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

        # í†µê³„ ì¶œë ¥
        stats = searcher.get_statistics()
        print(f"ğŸ“Š ì´ íŒë¡€ ìˆ˜: {stats['total_cases']:,}ê°œ")
        print(f"ğŸ“š ì–´íœ˜ í¬ê¸°: {stats['vocabulary_size']:,}ê°œ")

        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        test_query = "ì‘ì—… ì¤‘ ë‹¤ì¹œ ì†ê°€ë½"
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{test_query}'")

        results = searcher.search(test_query, top_k=3)
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result.title}")
            print(f"   ë²•ì›: {result.court}")
            print(f"   ìœ ì‚¬ë„: {result.similarity:.3f}")
            print(f"   ìš”ì•½: {result.content[:100]}...")

        # ë³´ê³ ì„œ ìƒì„±
        print(f"\nğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±...")
        report = searcher.generate_report(test_query)
        print(f"   í‰ê·  ìœ ì‚¬ë„: {report.get('average_similarity', 'N/A')}")
        print(f"   ê´€ë ¨ íŒë¡€ ìˆ˜: {report.get('total_results', 'N/A')}ê°œ")

    else:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")